{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモ：RMSDを比較することで、良い特徴量だけを選び出す。さらに、それらの良い特徴量だけで計算を行い、全部入りの計算結果と比較して性能差がどれくらいあるか調べる。性能差がそこまで変わらず、計算時間が減ったら嬉しい。KipfらによるSpatial GCNを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "import rdkit.Chem.AllChem as AllChem\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset, download_url, extract_zip\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import sys\n",
    "import shutil\n",
    "from typing import Callable, List, Optional\n",
    "import tqdm\n",
    "from math import sqrt as sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9csv = pd.read_csv(\"/home/higuchi/Pytorch/GCN/QM9/raw/gdb9.sdf.csv\")\n",
    "sdf = \"/home/higuchi/Pytorch/GCN/QM9/raw/gdb9.sdf\"\n",
    "#mols = rdkit.Chem.SDMolSupplier(sdf, removeHs=False) #sdfからmolオブジェクトを生成\n",
    "#mols = [m for m in mols if m is not None]\n",
    "with open(\"mols_unprocessed\", \"rb\") as f:\n",
    "    mols = pickle.load(f)\n",
    "\n",
    "# ETKDG Process (Mettya zikan kakaru...)\n",
    "mols_ETKDG = []\n",
    "mols_Hs = [rdkit.Chem.AddHs(mol) for mol in mols]\n",
    "for mol in mols_Hs:\n",
    "    ETKDG = AllChem.ETKDG()\n",
    "    AllChem.EmbedMolecule(mol, ETKDG)\n",
    "    mols_ETKDG.append(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D structure generation with DeepChem (Hayai! approx. 2m)\n",
    "import deepchem\n",
    "featurizer = deepchem.feat.Mol2VecFingerprint()\n",
    "mols_features = []\n",
    "for mol in mols_Hs:\n",
    "    features = featurizer.featurize(rdkit.Chem.MolToSmiles(mol))\n",
    "    mols_features.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting:\n",
    "layer=3, hidden_layer=64\n",
    "epoch_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.dataset = dataset\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, dataset, layer=3, dim=64):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dataset = dataset\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN3, self).__init__()\n",
    "        hidden_layer = 64\n",
    "        self.conv1 =  GCNConv(dataset.num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.conv3 = GCNConv(64, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        batch, x, edge_index = data.batch, data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "HAR2EV = 27.211386246\n",
    "KCALMOL2EV = 0.04336414\n",
    "\n",
    "conversion = torch.tensor([\n",
    "    1., 1., HAR2EV, HAR2EV, HAR2EV, 1., HAR2EV, HAR2EV, HAR2EV, HAR2EV, HAR2EV,\n",
    "    1., KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, 1., 1., 1.\n",
    "])\n",
    "\n",
    "atomrefs = {\n",
    "    6: [0., 0., 0., 0., 0.],\n",
    "    7: [\n",
    "        -13.61312172, -1029.86312267, -1485.30251237, -2042.61123593,\n",
    "        -2713.48485589\n",
    "    ],\n",
    "    8: [\n",
    "        -13.5745904, -1029.82456413, -1485.26398105, -2042.5727046,\n",
    "        -2713.44632457\n",
    "    ],\n",
    "    9: [\n",
    "        -13.54887564, -1029.79887659, -1485.2382935, -2042.54701705,\n",
    "        -2713.42063702\n",
    "    ],\n",
    "    10: [\n",
    "        -13.90303183, -1030.25891228, -1485.71166277, -2043.01812778,\n",
    "        -2713.88796536\n",
    "    ],\n",
    "    11: [0., 0., 0., 0., 0.],\n",
    "}\n",
    "\n",
    "class MyQM9(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    def mean(self, target: int) -> float:\n",
    "        y = torch.cat([self.get(i).y for i in range(len(self))], dim=0)\n",
    "        return float(y[:, target].mean())\n",
    "\n",
    "    def std(self, target: int) -> float:\n",
    "        y = torch.cat([self.get(i).y for i in range(len(self))], dim=0)\n",
    "        return float(y[:, target].std())\n",
    "\n",
    "    def atomref(self, target) -> Optional[torch.Tensor]:\n",
    "        if target in atomrefs:\n",
    "            out = torch.zeros(100)\n",
    "            out[torch.tensor([1, 6, 7, 8, 9])] = torch.tensor(atomrefs[target])\n",
    "            return out.view(-1, 1)\n",
    "        return None\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"data_v3.pt\"\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\n",
    "        bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "\n",
    "        #回帰ターゲット\n",
    "        df = pd.read_csv(\"qm9_dataset.csv\")\n",
    "        df_target = df.reindex(columns=[\"mu\", \"alpha\", \"homo\", \"lumo\", \"gap\", \"r2\", \"zpve\", \"u0\", \"u298\", \"h298\", \"g298\", \"cv\", \"u0_atom\", \"u298_atom\", \"h298_atom\", \"g298_atom\", \"A\", \"B\", \"C\"])\n",
    "        target = torch.tensor([list(i[1:]) for i in df_target.itertuples()], dtype=torch.float)\n",
    "        self.target = target\n",
    "\n",
    "        with open(\"./uncharacterized.txt\") as f:\n",
    "            #計算できんかったやつ\n",
    "            skip = [int(x.split()[0]) - 1 for x in f.read().split('\\n')[9:-2]]\n",
    "        \n",
    "        smiles = df[\"smiles\"].tolist()\n",
    "        mols = [Chem.MolFromSmiles(m) for m in smiles]\n",
    "        data_list = []\n",
    "        for i, mol in enumerate(tqdm.tqdm(mols)):\n",
    "            if i in skip: #計算できんかったやつを飛ばす\n",
    "                continue\n",
    "\n",
    "            mol = Chem.AddHs(mol)\n",
    "        \n",
    "            N = mol.GetNumAtoms() #分子の原子数\n",
    "            \n",
    "            conf = mol.GetConformers()\n",
    "\n",
    "            type_idx = []\n",
    "            atomic_number = []\n",
    "            formal_charge = []\n",
    "            valence = []\n",
    "            degree = []\n",
    "            aromatic = []\n",
    "            sp = []\n",
    "            sp2 = []\n",
    "            sp3 = []\n",
    "            num_hs = []\n",
    "\n",
    "            for atom in mol.GetAtoms():\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "                atomic_number.append(atom.GetAtomicNum())\n",
    "                formal_charge.append(atom.GetFormalCharge())\n",
    "                valence.append(atom.GetTotalValence())\n",
    "                degree.append(atom.GetTotalDegree())\n",
    "                aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
    "                hybridization = atom.GetHybridization()\n",
    "                sp.append(1 if hybridization == HybridizationType.SP else 0)\n",
    "                sp2.append(1 if hybridization == HybridizationType.SP2 else 0)\n",
    "                sp3.append(1 if hybridization == HybridizationType.SP3 else 0)\n",
    "                num_hs.append(atom.GetTotalNumHs(includeNeighbors=True))\n",
    "\n",
    "            row, col, edge_type = [], [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                edge_type += 2 * [bonds[bond.GetBondType()]]\n",
    "\n",
    "            edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "            edge_attr = one_hot(edge_type, num_classes=len(bonds))\n",
    "            perm = (edge_index[0] * N + edge_index[1]).argsort()\n",
    "            edge_index = edge_index[:, perm]\n",
    "            edge_type = edge_type[perm]\n",
    "            edge_attr = edge_attr[perm]\n",
    "\n",
    "            row, col = edge_index\n",
    "            \n",
    "            #x1 = one_hot(torch.tensor(type_idx), num_classes=len(types))\n",
    "            #x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs],\n",
    "                            #dtype=torch.float).t().contiguous()\n",
    "            desc_dict = {\n",
    "            \"atomic_number\":atomic_number,\n",
    "            \"formal_charge\":formal_charge,\n",
    "            \"valence\":valence,\n",
    "            \"degree\":degree,\n",
    "            \"aromatic\":aromatic,\n",
    "            \"sp\":sp,\n",
    "            \"sp2\":sp2,\n",
    "            \"sp3\":sp3,\n",
    "            \"num_hs\":num_hs\n",
    "            }\n",
    "            descriptors_in_use = [atomic_number, formal_charge, valence, degree, aromatic, sp, sp2, sp3, num_hs]\n",
    "            if pre_reduce:\n",
    "                print(pre_reduce)\n",
    "                descriptors_in_use.remove(desc_dict[pre_reduce])\n",
    "            x = torch.tensor(descriptors_in_use, dtype=torch.float).t().contiguous()\n",
    "            #x = torch.cat([x1, x2], dim=-1)\n",
    "            y = target[i].unsqueeze(0)\n",
    "            smiles = rdkit.Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "            data = Data(x=x, edge_index=edge_index, smiles=smiles, edge_attr=edge_attr, y=y, idx=i)\n",
    "            data_list.append(data)\n",
    "\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "rmse = RMSELoss()\n",
    "def train(initial_epoch_num=None, early_stopping=None):\n",
    "    #early_stopping = EarlyStopping(patience=1, verbose=True, path=f\"{pre_reduce}_best.pt\")\n",
    "    results = []\n",
    "    for epoch in range(epoch_num):\n",
    "        if initial_epoch_num > 0:\n",
    "            epoch += initial_epoch_num\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            loss = rmse(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss = train_loss / total_graphs #損失の平均(batchあたり) ルートを取ってから平均\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in valid_loader:\n",
    "            batch.to(device)\n",
    "            prediction = model(batch)\n",
    "            loss = rmse(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            valid_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "        valid_loss = valid_loss / total_graphs\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "        results.append({\"Epoch\":epoch+1, \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "plt.style.library[\"seaborn\"] = plt.style.library[\"seaborn-v0_8\"]\n",
    "vizdf = AutoViz_Class()\n",
    "filename = \"./qm9_dataset.csv\"\n",
    "graph = vizdf.AutoViz(\n",
    "    filename,\n",
    "    depVar=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "plt.hist(df[\"alpha\"].tolist(), range=(0,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "  3%|▎         | 4016/133885 [00:00<00:20, 6396.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMyQM9\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./QM9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m num_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     21\u001b[0m num_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m num_train\n",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m, in \u001b[0;36mMyQM9.__init__\u001b[0;34m(self, root, transform, pre_transform)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:76\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     log: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/dataset.py:102\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/dataset.py:235\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    234\u001b[0m makedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    238\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[0;32mIn[10], line 156\u001b[0m, in \u001b[0;36mMyQM9.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#x = torch.cat([x1, x2], dim=-1)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m y \u001b[38;5;241m=\u001b[39m target[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m smiles \u001b[38;5;241m=\u001b[39m \u001b[43mrdkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMolToSmiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misomericSmiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39medge_index, smiles\u001b[38;5;241m=\u001b[39msmiles, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, y\u001b[38;5;241m=\u001b[39my, idx\u001b[38;5;241m=\u001b[39mi)\n\u001b[1;32m    158\u001b[0m data_list\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#乱数ジェネレート\n",
    "descriptors = [None, \"atomic_number\", \"aromatic\", \"sp\", \"sp2\", \"sp3\", \"num_hs\"]\n",
    "device = \"cuda\"\n",
    "\n",
    "layer = 3\n",
    "dim = 64\n",
    "epoch_num = 100\n",
    "target_idx = 1\n",
    "batch_size = 32\n",
    "\n",
    "add_to_old_file = True\n",
    "for pre_reduce in descriptors:\n",
    "    filepath = f\"./results/1031/GCN_without_{pre_reduce}\"\n",
    "    try:\n",
    "        shutil.rmtree(\"./QM9\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    dataset = MyQM9(root=\"./QM9\")\n",
    "    num_train = int(len(dataset)*0.8)\n",
    "    num_val = len(dataset) - num_train\n",
    "    \n",
    "    # 乱数の固定\n",
    "    for i in range(1):\n",
    "        train_set, valid_set = random_split(dataset, [num_train, num_val])\n",
    "        #Dataloaderの生成\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                results_dict_old = pickle.load(f)\n",
    "                results_old = results_dict_old[\"results\"]\n",
    "                initial_epoch_num = int(results_old[-1][\"Epoch\"])\n",
    "                time_old = results_dict_old[\"time\"]\n",
    "        else:\n",
    "            initial_epoch_num = 0\n",
    "        \n",
    "        model = GCN3().to(device)\n",
    "        # Optimizerの初期化\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        old_file_exists = False\n",
    "        if os.path.isfile(filepath + \"_model\"):\n",
    "            old_file_exists = True\n",
    "            model.load_state_dict(torch.load(filepath + \"_model\"))\n",
    "            print(\"loaded old model\")\n",
    "        else:\n",
    "            print(\"using brand new model\")\n",
    "\n",
    "        start = time.time()\n",
    "        print(initial_epoch_num)\n",
    "        results = train(initial_epoch_num=initial_epoch_num) #RMSE\n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "\n",
    "        if old_file_exists:\n",
    "            results = results_old + results\n",
    "            diff += time_old\n",
    "        results_dict = {\"results\":results, \"time\":diff}\n",
    "        \n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        torch.save(model.state_dict(), filepath + \"_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/GCN_without_None\n"
     ]
    }
   ],
   "source": [
    "pre_reduce = None\n",
    "print(f\"./results/GCN_without_{pre_reduce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyQM9(root=\"./QM9_reduced/\")\n",
    "layer=3\n",
    "dim=64\n",
    "device=\"cuda\"\n",
    "model = GCN_N(dataset=dataset, layer=layer, dim=dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#消す記述子の選択\n",
    "descriptors = [\"atomic_number\", \"aromatic\", \"sp\", \"sp2\", \"sp3\", \"num_hs\"]\n",
    "for desc in descriptors:\n",
    "    print(desc)\n",
    "    try:\n",
    "        shutil.rmtree(\"./QM9_reduced/processed/\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    dataset_reduced = MyQM9(root=\"./QM9_reduced\")\n",
    "    dataset = dataset_reduced #必ずチェック！！\n",
    "    print(dataset[0].x.shape)\n",
    "\n",
    "    #データの分割(total: 130831)\n",
    "    num_train = int(len(dataset)*0.8)\n",
    "    num_val = len(dataset) - num_train\n",
    "    num_test = 0\n",
    "    batch_size = 64\n",
    "\n",
    "    # 乱数の固定\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    g = torch.Generator()\n",
    "    train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test], g.manual_seed(0))\n",
    "\n",
    "    #Dataloaderの生成\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, worker_init_fn=seed_worker)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=seed_worker)\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    \n",
    "    layer = 3\n",
    "    dim = 64\n",
    "    epoch_num = 100\n",
    "    target_idx = 1\n",
    "    mse = F.mse_loss\n",
    "\n",
    "    start = time.time()\n",
    "    results = train(mse, dataset=dataset) #RMSE\n",
    "    end = time.time()\n",
    "    diff = end-start\n",
    "\n",
    "    results = {\"results\":results, \"time\":diff}\n",
    "\n",
    "    with open(f\"./results/GCN_without_{desc}\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def plot_train(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        result = pickle.load(f)\n",
    "    plt.plot([i[\"train_loss\"] for i in result[\"results\"]], label=f\"training_{filename}\")\n",
    "\n",
    "def plot_valid(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        result = pickle.load(f)\n",
    "    plt.plot([i[\"valid_loss\"] for i in result[\"results\"]], label=f\"valid_{filename}\")\n",
    "\n",
    "def plot_train_errorbar(filepath, label=None):\n",
    "    files = glob.glob(f\"{filepath}_*\")\n",
    "    result_list = []\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "            result = [i[\"train_loss\"] for i in result[\"results\"]]\n",
    "        result_list.append(result)\n",
    "    result_avg = np.mean(result_list, axis=0)\n",
    "    result_error_upper = [max([result[i] for result in result_list]) - result_avg[i] for i in range(len(result_avg))]\n",
    "    result_error_lower = [abs(min([result[i] for result in result_list]) - result_avg[i]) for i in range(len(result_avg))]\n",
    "    plt.errorbar(x=[i for i in range(len(result_avg))] , y=result_avg, yerr=[result_error_upper, result_error_lower ])\n",
    "    plt.plot(result_avg, label=label)\n",
    "\n",
    "def plot_valid_errorbar(filepath):\n",
    "    files = glob.glob(f\"{filepath}_*\")\n",
    "    result_list = []\n",
    "    for file in files:\n",
    "        with open(file, \"rb\") as f:\n",
    "            result = pickle.load(f)\n",
    "            result = [i[\"valid_loss\"] for i in result[\"results\"]]\n",
    "        result_list.append(result)\n",
    "    result_avg = np.mean(result_list, axis=0)\n",
    "    result_error_upper = [max([result[i] for result in result_list]) - result_avg[i] for i in range(len(result_avg))]\n",
    "    result_error_lower = [abs(min([result[i] for result in result_list]) - result_avg[i]) for i in range(len(result_avg))]\n",
    "    plt.errorbar(result_avg, yerr=[result_error_upper, result_error_lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc in descriptors:\n",
    "    plot_train_errorbar(f\"./results/test/GCN_without_{desc}\", label=desc)\n",
    "plt.ylim([1.0,2.0])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/GCN_all\", \"rb\") as f:\n",
    "    result_all = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_atomic_number\", \"rb\") as f:\n",
    "    result_atomic_number = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_aromatic\", \"rb\") as f:\n",
    "    result_aromatic = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_sp\", \"rb\") as f:\n",
    "    result_sp = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_sp2\", \"rb\") as f:\n",
    "    result_sp2 = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_sp3\", \"rb\") as f:\n",
    "    result_sp3 = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_num_hs\", \"rb\") as f:\n",
    "    result_sp3 = pickle.load(f)\n",
    "\n",
    "with open(\"./results/GCN_without_num_hs\", \"rb\") as f:\n",
    "    result_num_hs = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./results/test/GCN_without_aromatic*\")\n",
    "result_list = []\n",
    "for file in files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        result = pickle.load(f)\n",
    "        result = [i[\"train_loss\"] for i in result[\"results\"]]\n",
    "    result_list.append(result)\n",
    "result_avg = np.mean(result_list, axis=0)\n",
    "result_error_upper = [max([result[i] for result in result_list]) - result_avg[i] for i in range(len(result_avg))]\n",
    "result_error_lower = [abs(min([result[i] for result in result_list]) - result_avg[i]) for i in range(len(result_avg))]\n",
    "plt.errorbar(x=[i for i in range(len(result_avg))] , y=result_avg, yerr=[result_error_upper, result_error_lower ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x: ノード特徴量\n",
    "\n",
    "y: 正解ラベル\n",
    "\n",
    "pos: 原子の座標\n",
    "\n",
    "edge_index: エッジインデックス\n",
    "\n",
    "edge_attr: エッジ特徴量(使えん)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノード特徴量\n",
    "type_idx: 原子の種類 \n",
    "aromatic: 芳香性\n",
    "sp: sp混成\n",
    "sp2: sp2混成\n",
    "sp3: sp3混成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "data = dataset[100]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "#原子番号追加\n",
    "elements = {\n",
    "    1:\"H\",\n",
    "    2:\"He\",\n",
    "    3:\"Li\",\n",
    "    4:\"Be\",\n",
    "    5:\"B\",\n",
    "    6:\"C\",\n",
    "    7:\"N\",\n",
    "    8:\"O\",\n",
    "    9:\"F\"\n",
    "}\n",
    "elem_labels = {}\n",
    "for i in range(data.num_nodes):\n",
    "    elem = elements[int(data.z[i])]\n",
    "    elem_labels[i] = elem\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "networkx.draw_networkx_nodes(nxg,\n",
    "                            draw_position,\n",
    "                            node_size=[v / pagerank_max * 1000 for v in pagerank.values()]\n",
    "                            )\n",
    "\n",
    "networkx.draw_networkx_edges(nxg, draw_position, arrowstyle='-', alpha=0.2)\n",
    "networkx.draw_networkx_labels(nxg, draw_position, elem_labels, font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
