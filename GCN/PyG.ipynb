{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qm9' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m#dataset = QM9(root=\"./QM9\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#無向グラフの例\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m#data = Data(x=x, edge_index=edge_index) # コンストラクタ\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Data(x=[3, 1], edge_index=[2, 4])\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m dataset \u001b[39m=\u001b[39m qm9\u001b[39m.\u001b[39mQM9(root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./QM9\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Cheatsheet\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qm9' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "#import qm9\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])\n",
    "dataset = qm9.QM9(root=\"./QM9\")\n",
    "\n",
    "# Cheatsheet\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "alpha = df[\"alpha\"].tolist()\n",
    "alpha.index(max(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\"\"\n",
    "1 4.969047893 2.907558784\n",
    "2 2.475646626 6.965070182\n",
    "3 2.322402548 3.12543573\n",
    "4 2.219092184 2.137037913\n",
    "5 2.180948359 3.310040931\n",
    "6 2.15226011 2.081435579\n",
    "7 2.041437492 2.556189786\n",
    "8 1.934097374 1.933049785\n",
    "9 1.832175324 1.984427364\n",
    "10 1.787891822 1.817273377\n",
    "11 1.73516102 1.934816865\n",
    "12 1.729788833 1.979212807\n",
    "13 1.706661131 2.177934801\n",
    "14 1.703376503 2.401264786\n",
    "15 1.669216804 1.68484123\n",
    "16 1.655384878 1.715670932\n",
    "17 1.653513617 1.757639903\n",
    "18 1.646621699 2.308292219\n",
    "19 1.627009154 1.979533452\n",
    "20 1.633170295 1.940908777\n",
    "21 1.603458978 1.957588522\n",
    "22 1.609296356 1.939367837\n",
    "23 1.616707744 1.718332977\n",
    "24 1.603558939 1.74042602\n",
    "25 1.604870725 2.213842579\n",
    "26 1.594901907 1.697182396\n",
    "27 1.597781276 1.680925577\n",
    "28 1.578765319 1.755495607\n",
    "29 1.572161565 1.742309279\n",
    "30 1.602938531 1.651008191\n",
    "31 1.624461315 1.609181932\n",
    "32 1.579808177 1.770808932\n",
    "33 1.58385286 1.636422551\n",
    "34 1.569753096 1.608233983\n",
    "35 1.581631238 2.028857191\n",
    "36 1.578716519 1.77206033\n",
    "37 1.59089365 1.947579651\n",
    "38 1.573185929 1.995602528\n",
    "39 1.564494636 2.082876043\n",
    "40 1.573648101 1.617239682\n",
    "41 1.599268679 1.697913339\n",
    "42 1.559035714 1.794609357\n",
    "43 1.562170836 2.606648212\n",
    "44 1.547039565 1.720932212\n",
    "45 1.576562542 1.740025243\n",
    "46 1.551760188 1.659042093\n",
    "47 1.563448262 2.054811586\n",
    "48 1.541001491 1.686982509\n",
    "49 1.565207518 2.469556225\n",
    "50 1.552765122 2.129995664\n",
    "\"\"\"\n",
    "result = result.split(\"\\n\")[1:]\n",
    "result.pop(-1)\n",
    "result_train = []\n",
    "result_valid = []\n",
    "for r in result:\n",
    "    r = r.split(\" \")\n",
    "    result_train.append(float(r[1]))\n",
    "    result_valid.append(float(r[2]))\n",
    "plt.plot([i for i in range(1, 51)], result_train, label=\"train_loss\")\n",
    "plt.plot([i for i in range(1, 51)], result_valid, label=\"validation_loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss(RMSE)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "    \n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "\n",
    "# 説明変数と説明変数(α)\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=64, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "error_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80]\n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "list(df[\"smiles\"])\n",
    "atomrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyFirstDataset(root=\"MyFirstDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, i in df.iterrows():\n",
    "    target = [float(x) for x in i.values[2:]]\n",
    "    target = torch.tensor(target, dtype=torch.float)\n",
    "    #target = torch.cat([target[:, 3:], target[:, 3:]], dim=0)\n",
    "    #target = target * conversion.view(1, -1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n\u001b[1;32m     25\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 27\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGCN_N\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     28\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, layer:\u001b[39mint\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, dataset\u001b[39m=\u001b[39mdataset):\n\u001b[1;32m     29\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mGCN_N\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGCN_N\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, layer:\u001b[39mint\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, dataset\u001b[39m=\u001b[39mdataset):\n\u001b[1;32m     29\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     30\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m layer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32, dataset=dataset):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dataset = dataset\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GAT_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATConv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATConv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class trans_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr[0])\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr[0])\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GNNModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.6), int(len(dataset)*0.2)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "layer = 3\n",
    "dim = 32\n",
    "model = GCN_N(layer=layer,dim=dim)\n",
    "#model = trans_N(layer=layer,dim=dim)\n",
    "# 損失関数\n",
    "criterion = F.mse_loss\n",
    "# Optimizerの初期化\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(params=lr=0.01, weight_decay=5e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "\n",
    "target_idx = 1\n",
    "num_epochs = 10\n",
    "fold = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "for f, (train_idx,valid_idx) in enumerate(fold.split(np.arange(len(dataset)))):\n",
    "    \n",
    "    print(f\"Fold {f + 1}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        train_loss = sqrt(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_loss = sqrt(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m r2_score\n\u001b[1;32m      5\u001b[0m target_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 6\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m#時間計測開始\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[39m# train\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "target_idx = 1\n",
    "start = time.time() #時間計測開始\n",
    "for epoch in range(5):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_graphs = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(batch)\n",
    "        #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "        loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "    train_loss = sqrt(train_loss)\n",
    "    \n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    total_graphs = 0\n",
    "    with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "        for batch in valid_loader:\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            valid_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_loss = sqrt(valid_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "    #loss_three_50.append({\"Epoch\":epoch + 1 , \"train_loss\":train_loss, \"valid_loss\":valid_loss})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ層の数は層によって変えるべき？\n",
    "\n",
    "edge_attr:多次元\n",
    "\n",
    "edge_weight:一次元\n",
    "\n",
    "GCNはedge_weightのため、QM9のedge_attrが使えない。そのため、edge_attrなしでの計算になる\n",
    "\n",
    "GATはedge_attrが使える。edge_attrの追加によって悪化した。\n",
    "GAT2はedge_attrが使えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体的な予[測値\n",
    "[[prediction[i].item(),batch.y[:, target_idx][i].item()] for i in range(len(prediction))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
