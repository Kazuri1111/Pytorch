{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])\n",
    "\n",
    "# Cheatsheet\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "alpha = df[\"alpha\"].tolist()\n",
    "alpha.index(max(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "    \n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "\n",
    "# 説明変数と説明変数(α)\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=64, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "error_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80]\n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "list(df[\"smiles\"])\n",
    "atomrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyFirstDataset(root=\"MyFirstDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, i in df.iterrows():\n",
    "    target = [float(x) for x in i.values[2:]]\n",
    "    target = torch.tensor(target, dtype=torch.float)\n",
    "    #target = torch.cat([target[:, 3:], target[:, 3:]], dim=0)\n",
    "    #target = target * conversion.view(1, -1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32, dataset=dataset):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dataset = dataset\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(1, self.layer):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GAT_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATConv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATConv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class trans_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr[0])\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr[0])\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GNNModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.6), int(len(dataset)*0.2)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "layer = 2\n",
    "dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "Epoch 1 | train_loss:4.724925012052014, valid_loss:2.9206775460335\n",
      "Epoch 2 | train_loss:2.420287570888619, valid_loss:2.2848135492017185\n",
      "Epoch 3 | train_loss:2.3424044917417266, valid_loss:2.2120257789819573\n",
      "\n",
      "MAE\n",
      "Epoch 1 | train_loss:2.328556866109517, valid_loss:1.774058752508793\n",
      "Epoch 2 | train_loss:1.787722352032945, valid_loss:1.6820572037568593\n",
      "Epoch 3 | train_loss:1.7385893689002463, valid_loss:1.5303391531510575\n"
     ]
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "epoch_num = 3\n",
    "target_idx = 1 # 0はじまり\n",
    "targets = [\"Isotropic polarizability\", \"HOMO\", \"LUMO\", \"E_Gap\", \"Electronic spatial extent\", \"ZPVE\", \"U_0\", \"U\", \"H\", \"G\", \"Cv\", \"U_0 ATOM\", \"U ATOM\", \"H ATOM\", \"G ATOM\", \"A\", \"B\", \"C\"]\n",
    "target_name = targets[target_idx]\n",
    "start = time.time() #時間計測開始\n",
    "results = []\n",
    "\n",
    "mse = F.mse_loss\n",
    "mae = F.l1_loss #mae\n",
    "\n",
    "def train(criterion):\n",
    "    # 学習前に毎回実行する\n",
    "    model = GCN_N(layer=layer,dim=dim)\n",
    "    # Optimizerの初期化\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(epoch_num):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        if criterion == mse:\n",
    "            train_loss = sqrt(train_loss)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        if criterion == mse:\n",
    "            valid_loss = sqrt(valid_loss)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "        results.append({\"Epoch\":epoch+1, \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "    return results\n",
    "\n",
    "print(\"RMSE\")\n",
    "results_mse = train(criterion=mse)\n",
    "print(\"\")\n",
    "\n",
    "print(\"MAE\")\n",
    "results_mae = train(criterion=mae)\n",
    "\n",
    "results = [{\"Epoch\":i + 1, \"train_loss_RMSE\":results_mse[i][\"train_loss\"], \"valid_loss_RMSE\":results_mse[i][\"valid_loss\"], \"train_loss_MAE\":results_mae[i][\"train_loss\"], \"valid_loss_MAE\":results_mae[i][\"valid_loss\"]} for i in range(epoch_num)]\n",
    "\n",
    "with open(f\"./results/GCN_{target_name}_{layer}_{dim}_{epoch_num}\", \"wb\") as f: #ファイル名：ターゲット、層数、隠れ層数、エポック数 (can be loaded by pickle)\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Epoch': 1,\n",
       "  'train_loss': 1.1846697023198798,\n",
       "  'valid_loss': 1.189590138897659},\n",
       " {'Epoch': 2,\n",
       "  'train_loss': 1.1830113312231954,\n",
       "  'valid_loss': 1.258389941926049},\n",
       " {'Epoch': 3,\n",
       "  'train_loss': 1.1805809141107069,\n",
       "  'valid_loss': 1.1644323207260707}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/GCN_Isotropic polarizability_2_32_2\", \"rb\") as f:\n",
    "    test1 = pickle.load(f)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newarray = np.ndarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "\n",
    "target_idx = 1\n",
    "num_epochs = 10\n",
    "fold = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "for f, (train_idx,valid_idx) in enumerate(fold.split(np.arange(len(dataset)))):\n",
    "    \n",
    "    print(f\"Fold {f + 1}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        train_loss = sqrt(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_loss = sqrt(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.5111434600112155,\n",
       " 3.4445386044421165,\n",
       " 3.4405643966218706,\n",
       " 3.437776689995181,\n",
       " 3.418383062939326]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"pickle_test\",\"rb\") as f:\n",
    "    pickle_test = pickle.load(f)\n",
    "pickle_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ層の数は層によって変えるべき？\n",
    "\n",
    "edge_attr:多次元\n",
    "\n",
    "edge_weight:一次元\n",
    "\n",
    "GCNはedge_weightのため、QM9のedge_attrが使えない。そのため、edge_attrなしでの計算になる\n",
    "\n",
    "GATはedge_attrが使える。edge_attrの追加によって悪化した。\n",
    "GAT2はedge_attrが使えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体的な予[測値\n",
    "[[prediction[i].item(),batch.y[:, target_idx][i].item()] for i in range(len(prediction))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
