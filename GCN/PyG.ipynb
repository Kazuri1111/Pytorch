{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "import qm9\n",
    "\n",
    "#dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])\n",
    "dataset = qm9.QM9(root=\"./QM9\")\n",
    "# Cheatsheet\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "    \n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubchempy\n",
    "test1 = pubchempy.get_properties(\"CanonicalSMILES\", \"MDMA\", \"name\")\n",
    "test2 = pubchempy.get_properties(\"CanonicalSMILES\", \"amphetamine\", \"name\")\n",
    "smiles1 = test1[0][\"CanonicalSMILES\"]\n",
    "smiles2 = test2[0][\"CanonicalSMILES\"]\n",
    "rdkit.DataStructs.FingerprintSimilarity(ECFPGen(smiles1), ECFPGen(smiles2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "\n",
    "# 説明変数と説明変数(α)\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=4, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "error_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80]\n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./qm9_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mlist\u001b[39m(df[\u001b[39m\"\u001b[39m\u001b[39msmiles\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[39mtype\u001b[39m(H)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "list(df[\"smiles\"])\n",
    "atomrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m MyFirstDataset(root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMyFirstDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[110], line 41\u001b[0m, in \u001b[0;36mMyFirstDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./MyFirstDataset\u001b[39m\u001b[39m\"\u001b[39m, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pre_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pre_filter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter)\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:97\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_process:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:230\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m    229\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[1;32m    232\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    233\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[0;32mIn[110], line 96\u001b[0m, in \u001b[0;36mMyFirstDataset.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     sp2\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m hybridization \u001b[39m==\u001b[39m HybridizationType\u001b[39m.\u001b[39mSP2 \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m     sp3\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m hybridization \u001b[39m==\u001b[39m HybridizationType\u001b[39m.\u001b[39mSP3 \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(atomic_number, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong) \u001b[39m#原子番号\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# edge index\u001b[39;00m\n\u001b[1;32m     99\u001b[0m row, col, edge_type \u001b[39m=\u001b[39m [], [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = MyFirstDataset(root=\"MyFirstDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, i in df.iterrows():\n",
    "    target = [float(x) for x in i.values[2:]]\n",
    "    target = torch.tensor(target, dtype=torch.float)\n",
    "    #target = torch.cat([target[:, 3:], target[:, 3:]], dim=0)\n",
    "    #target = target * conversion.view(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "mol_obj = mol\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "edge_type = []\n",
    "for bond in mol.GetBonds():\n",
    "    edge_type += 2 * [bonds[bond.GetBondType()]]\n",
    "edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "edge_type = one_hot(edge_type)\n",
    "row, col, edge_type = [], [], []\n",
    "for bond in mol_obj.GetBonds():\n",
    "    start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "    row += [start, end]\n",
    "    col += [end, start]\n",
    "    edge_type += 2 * [bonds[bond.GetBondType()]]\n",
    "\n",
    "edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "edge_type = torch.tensor(edge_type, dtype=torch.long) #結合次数\n",
    "edge_attr = one_hot(edge_type, num_classes=len(bonds)) #one hot vector: vector that has 0 or 1 for each value\n",
    "\n",
    "perm = (edge_index[0] * 8 + edge_index[1]).argsort()\n",
    "edge_index = edge_index[:, perm]\n",
    "edge_type = edge_type[perm]\n",
    "edge_attr = edge_attr[perm]\n",
    "edge_type\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GAT_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATConv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATConv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class trans_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr[0])\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr[0])\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 4.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GNNModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset作成\n",
    "polar = df[\"alpha\"]\n",
    "ECFP = df[\"ECFP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.6), int(len(dataset)*0.2)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "layer = 3\n",
    "dim = 32\n",
    "model = GCN_N(layer=layer,dim=dim)\n",
    "#model = trans_N(layer=layer,dim=dim)\n",
    "# 損失関数\n",
    "criterion = F.mse_loss\n",
    "# Optimizerの初期化\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(params=lr=0.01, weight_decay=5e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 | train_loss:4.969047893359769, valid_loss:2.907558783754285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m prediction \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     15\u001b[0m \u001b[39m#loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m criterion(prediction, batch\u001b[39m.\u001b[39my[:, target_idx]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[133], line 41\u001b[0m, in \u001b[0;36mGCN_N.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     39\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvn(x, edge_index)\n\u001b[1;32m     42\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     43\u001b[0m x \u001b[39m=\u001b[39m pyg\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mglobal_add_pool(x, batch) \n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:232\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x)\n\u001b[1;32m    231\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_weight\u001b[39m=\u001b[39;49medge_weight,\n\u001b[1;32m    233\u001b[0m                      size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:459\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m decomp_args:\n\u001b[1;32m    457\u001b[0m         kwargs[arg] \u001b[39m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 459\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_args, edge_index, size,\n\u001b[1;32m    460\u001b[0m                           kwargs)\n\u001b[1;32m    462\u001b[0m msg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    463\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:336\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    335\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_size(size, dim, data)\n\u001b[0;32m--> 336\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lift(data, edge_index, dim)\n\u001b[1;32m    338\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m~/mambaforge/envs/torchenv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:272\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    273\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m index\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "from math import sqrt\n",
    " \n",
    "target_idx = 1\n",
    "start = time.time() #時間計測開始\n",
    "for epoch in range(50):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_graphs = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(batch)\n",
    "        #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "        loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "    train_loss = sqrt(train_loss)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    total_graphs = 0\n",
    "    with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "        for batch in valid_loader:\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            valid_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_loss = sqrt(valid_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1+50} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "    #loss_three_50.append({\"Epoch\":epoch + 1 , \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "used_time = time.time() - start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ層の数は層によって変えるべき？\n",
    "\n",
    "edge_attr:多次元\n",
    "\n",
    "edge_weight:一次元\n",
    "\n",
    "GCNはedge_weightのため、QM9のedge_attrが使えない。そのため、edge_attrなしでの計算になる\n",
    "\n",
    "GATはedge_attrが使える。edge_attrの追加によって悪化した。\n",
    "GAT2はedge_attrが使えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_two_50 = loss_three_50[51:]\n",
    "\n",
    "loss_two_50.append(\n",
    "    {\n",
    "        \"model\": \"GCN\",\n",
    "        \"layer\": layer,\n",
    "        \"dim\": dim,\n",
    "        \"batch_size\": 32,\n",
    "        \"loss\": \"RMSE\",\n",
    "        \"lr\": 0.01,\n",
    "        \"decay\": 5e-4,\n",
    "        \"seed\": seed,\n",
    "        \"data_split\":[\n",
    "            0.6,\n",
    "            0.6,\n",
    "            0.2\n",
    "        ],\n",
    "        \"time\": round(used_time, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./loss_two_50.json\", \"a\") as f:\n",
    "    json.dump(loss_two_50, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./loss_two_50.json\", \"r\") as f:\n",
    "    two = json.load(f)\n",
    "dim16 = two[1][:-1]\n",
    "dim32 = two[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
