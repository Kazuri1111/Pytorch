{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QM9' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/Pytorch/GCN/PyG.ipynb セル 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m dataset \u001b[39m=\u001b[39m QM9(root\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./QM9\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#無向グラフの例\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Cheatsheet\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:302\u001b[0m, in \u001b[0;36mInMemoryDataset.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    299\u001b[0m         data_list \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices()]\n\u001b[1;32m    300\u001b[0m         \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39mfrom_data_list(data_list)[key]\n\u001b[0;32m--> 302\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mattribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QM9' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])\n",
    "\n",
    "# Cheatsheet\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "\n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)\n",
    "\n",
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "\n",
    "#ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "with open(\"QM9_ECFP\", \"rb\") as f:\n",
    "    ECFP = pickle.load(f)\n",
    "# 説明変数と説明変数\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values # 双極子モーメント\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 2\n",
    "nBits = 2048\n",
    "ECFP = [list(ECFPGen(smiles, radius=radius, nBits=nBits)) for smiles in df[\"smiles\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (100,)\n",
    "mlp = MLPRegressor(max_iter=10000, activation=\"relu\", solver=\"adam\", verbose=True, hidden_layer_sizes=hidden_layer_sizes, early_stopping=True)\n",
    "mlp.loss = \"squared_error\"\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values #alpha:分極率 idx=1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "r2 = []\n",
    "start = time.time()\n",
    "mlp.fit(X_train,Y_train)\n",
    "end = time.time()\n",
    "time_diff = end - start\n",
    "with open(f\"results/ECFP_alpha_{radius}_{nBits}_{hidden_layer_sizes}\", \"wb\") as f:\n",
    "    rmse = [math.sqrt(i*2) for i in mlp.loss_curve_] #scikit-learnのMSEは2で割られているので、2をかけてから平方根を取る。\n",
    "    result = [rmse, mlp.validation_scores_, time_diff]\n",
    "    pickle.dump(mlp.loss_curve_, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#仮word用\n",
    "with open(\"results/ECFP_alpha_2_2048_(100,)\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(max_iter=1000, activation=\"relu\", solver=\"adam\", verbose=True, hidden_layer_sizes=hidden_layer_sizes)\n",
    "mlp.loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=64, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80] \n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "list(df[\"smiles\"])\n",
    "atomrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyFirstDataset(root=\"MyFirstDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, i in df.iterrows():\n",
    "    target = [float(x) for x in i.values[2:]]\n",
    "    target = torch.tensor(target, dtype=torch.float)\n",
    "    #target = torch.cat([target[:, 3:], target[:, 3:]], dim=0)\n",
    "    #target = target * conversion.view(1, -1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32, dataset=dataset):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dataset = dataset\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(1, self.layer):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GAT_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATConv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATConv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class trans_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr[0])\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr[0])\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GNNModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.8), int(len(dataset)*0.1)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)y\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "#layer = 2\n",
    "#dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "Epoch 1 | train_loss:4.00940383532039, valid_loss:2.4108854592902156\n",
      "Epoch 2 | train_loss:2.4619924735415712, valid_loss:2.2811358342432304\n",
      "Epoch 3 | train_loss:2.2578335386140123, valid_loss:2.0170335248332325\n",
      "Epoch 4 | train_loss:2.0302743969865253, valid_loss:1.710263311597712\n",
      "Epoch 5 | train_loss:1.924151710004786, valid_loss:1.937818917073988\n",
      "Epoch 6 | train_loss:1.8247654948804979, valid_loss:1.547212273062886\n",
      "Epoch 7 | train_loss:1.757406932542751, valid_loss:1.539213673384362\n",
      "Epoch 8 | train_loss:1.7286815503760236, valid_loss:1.4938697441499929\n",
      "Epoch 9 | train_loss:1.6792915832738438, valid_loss:1.7347841620653854\n",
      "Epoch 10 | train_loss:1.6991740955156118, valid_loss:1.456029488344603\n",
      "Epoch 11 | train_loss:1.6723103534945156, valid_loss:2.0334137950324753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/Pytorch/GCN/PyG.ipynb セル 40\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRMSE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m results_mse \u001b[39m=\u001b[39m train(criterion\u001b[39m=\u001b[39;49mmse)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m time_diff \u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m start\n",
      "\u001b[1;32m/home/higuchi/Pytorch/GCN/PyG.ipynb セル 40\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m total_graphs \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, OnDiskDataset):\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmulti_get(batch))\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/loader/dataloader.py:28\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(\n\u001b[1;32m     29\u001b[0m         batch,\n\u001b[1;32m     30\u001b[0m         follow_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[1;32m     31\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[1;32m     94\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m     95\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[1;32m     96\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     97\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[1;32m     98\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[1;32m     99\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[1;32m    103\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/collate.py:115\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m# In case the storage holds node, we add a top-level batch vector it:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m (add_batch \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(stores[\u001b[39m0\u001b[39m], NodeStorage)\n\u001b[1;32m    114\u001b[0m         \u001b[39mand\u001b[39;00m stores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcan_infer_num_nodes):\n\u001b[0;32m--> 115\u001b[0m     repeats \u001b[39m=\u001b[39m [store\u001b[39m.\u001b[39;49mnum_nodes \u001b[39mfor\u001b[39;49;00m store \u001b[39min\u001b[39;49;00m stores]\n\u001b[1;32m    116\u001b[0m     out_store\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m repeat_interleave(repeats, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    117\u001b[0m     out_store\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m cumsum(torch\u001b[39m.\u001b[39mtensor(repeats, device\u001b[39m=\u001b[39mdevice))\n",
      "File \u001b[0;32m~/micromamba/envs/Graph/lib/python3.11/site-packages/torch_geometric/data/collate.py:115\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m# In case the storage holds node, we add a top-level batch vector it:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m (add_batch \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(stores[\u001b[39m0\u001b[39m], NodeStorage)\n\u001b[1;32m    114\u001b[0m         \u001b[39mand\u001b[39;00m stores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcan_infer_num_nodes):\n\u001b[0;32m--> 115\u001b[0m     repeats \u001b[39m=\u001b[39m [store\u001b[39m.\u001b[39mnum_nodes \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m stores]\n\u001b[1;32m    116\u001b[0m     out_store\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m repeat_interleave(repeats, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    117\u001b[0m     out_store\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m cumsum(torch\u001b[39m.\u001b[39mtensor(repeats, device\u001b[39m=\u001b[39mdevice))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "epoch_num = 50\n",
    "target_idx = 1 # 0はじまり 0→分極率\n",
    "#targets = [\"Isotropic polarizability\", \"HOMO\", \"LUMO\", \"E_Gap\", \"Electronic spatial extent\", \"ZPVE\", \"U_0\", \"U\", \"H\", \"G\", \"Cv\", \"U_0 ATOM\", \"U ATOM\", \"H ATOM\", \"G ATOM\", \"A\", \"B\", \"C\"]\n",
    "#target_name = targets[target_idx]\n",
    "start = time.time() #時間計測開始\n",
    "results = []\n",
    "\n",
    "mse = F.mse_loss\n",
    "mae = F.l1_loss #mae\n",
    "\n",
    "def train(criterion):\n",
    "    # 学習前に毎回実行する\n",
    "    model = GCN_N(layer=layer,dim=dim).to(\"cuda\")\n",
    "    # Optimizerの初期化\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(epoch_num):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        if criterion == mse:\n",
    "            train_loss = sqrt(train_loss)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(\"cuda\")\n",
    "                prediction = model(batch)\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        if criterion == mse:\n",
    "            valid_loss = sqrt(valid_loss)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "        results.append({\"Epoch\":epoch+1, \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "    return results\n",
    "\n",
    "#for layer in range(2,6):\n",
    "#    for i in range(5,8):\n",
    "#        dim = 2**i\n",
    "for layer in range(4,5):\n",
    "    for dim in [128]:    \n",
    "        print(\"RMSE\")\n",
    "        start = time.time()\n",
    "        results_mse = train(criterion=mse)\n",
    "        end = time.time()\n",
    "        time_diff = end - start\n",
    "        results_mse = (results_mse, time_diff)\n",
    "        #print(\"\")\n",
    "        #print(\"MAE\")\n",
    "        #results_mae = train(criterion=mae)\n",
    "\n",
    "        #results = [{\"Epoch\":i + 1, \"train_loss_RMSE\":results_mse[i][\"train_loss\"], \"valid_loss_RMSE\":results_mse[i][\"valid_loss\"], \"accuracy\":accuracy} for i in range(epoch_num)]\n",
    "        target_name = \"dipole\"\n",
    "        with open(f\"./results/GCN_{target_name}_{layer}_{dim}_{epoch_num}\", \"wb\") as f: #ファイル名：ターゲット、層数、隠れ層数、エポック数 (can be loaded by pickle)\n",
    "            #pickle.dump(results, f)\n",
    "            pickle.dump(results_mse, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9069e+04,  4.5719e-41]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(1,2).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/GCN_dipole_4_128_50\",\"rb\") as f:\n",
    "    a=pickle.load(f)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/GCN_dipole_2_32_2\", \"rb\") as f:\n",
    "    test1 = pickle.load(f)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newarray = np.ndarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "\n",
    "target_idx = 1\n",
    "num_epochs = 10\n",
    "fold = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "for f, (train_idx,valid_idx) in enumerate(fold.split(np.arange(len(dataset)))):\n",
    "    \n",
    "    print(f\"Fold {f + 1}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        train_loss = sqrt(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_loss = sqrt(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickle_test\",\"rb\") as f:\n",
    "    pickle_test = pickle.load(f)\n",
    "pickle_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ層の数は層によって変えるべき？\n",
    "\n",
    "edge_attr:多次元\n",
    "\n",
    "edge_weight:一次元\n",
    "\n",
    "GCNはedge_weightのため、QM9のedge_attrが使えない。そのため、edge_attrなしでの計算になる\n",
    "\n",
    "GATはedge_attrが使える。edge_attrの追加によって悪化した。\n",
    "GAT2はedge_attrが使えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体的な予[測値\n",
    "[[prediction[i].item(),batch.y[:, target_idx][i].item()] for i in range(len(prediction))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/Pytorch/GCN/PyG.ipynb セル 43\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_networkx\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m SVG, display\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m dataset[\u001b[39m4921\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m nxg \u001b[39m=\u001b[39m to_networkx(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.142/home/higuchi/Pytorch/GCN/PyG.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m pagerank \u001b[39m=\u001b[39m networkx\u001b[39m.\u001b[39mpagerank(nxg) \u001b[39m#pagerankはノードの中心性(重要性の指標)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
