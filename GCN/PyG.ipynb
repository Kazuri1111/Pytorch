{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as pltf\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])\n",
    "\n",
    "# Cheatsheet\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7549693971410044,\n",
       " 0.5258810459281046,\n",
       " 0.42334398801759776,\n",
       " 0.33663386644526394,\n",
       " 0.2676507952913803,\n",
       " 0.2168752993866231,\n",
       " 0.17909847357777275,\n",
       " 0.1515371296466163,\n",
       " 0.13018745233290496,\n",
       " 0.11450019037260824,\n",
       " 0.10156003579119198,\n",
       " 0.09092828723707345,\n",
       " 0.08215253552807493,\n",
       " 0.07534602743568328,\n",
       " 0.06891527413871276,\n",
       " 0.0644856414191259,\n",
       " 0.05974203672705279,\n",
       " 0.055921451283512266,\n",
       " 0.0524036343821062,\n",
       " 0.04968474026481879,\n",
       " 0.04698213448618563,\n",
       " 0.04507975037973188,\n",
       " 0.04292402395186352,\n",
       " 0.040919855127502666,\n",
       " 0.03886705331416576,\n",
       " 0.03730199340863301,\n",
       " 0.035948192005908416,\n",
       " 0.034491618399767,\n",
       " 0.03330979410795313,\n",
       " 0.03168201980461464,\n",
       " 0.030708505135700042,\n",
       " 0.029947337567925128,\n",
       " 0.02850300041749049,\n",
       " 0.028143440104455308,\n",
       " 0.02727923747052591,\n",
       " 0.026273169046534747,\n",
       " 0.025544780150793277,\n",
       " 0.02461622851149089,\n",
       " 0.024098703728863136,\n",
       " 0.023581528403934456,\n",
       " 0.02271468855226151,\n",
       " 0.022136972581923584,\n",
       " 0.02137406213087631,\n",
       " 0.021002260549487603,\n",
       " 0.02027916083296518,\n",
       " 0.019933045348514755,\n",
       " 0.019721887725393263,\n",
       " 0.01913079938713374,\n",
       " 0.018761501754758225,\n",
       " 0.018417513369144094,\n",
       " 0.017877601000005416,\n",
       " 0.017586990863516113,\n",
       " 0.01731866991611784,\n",
       " 0.01749244537674074,\n",
       " 0.016812548755705793,\n",
       " 0.016328375377881593,\n",
       " 0.01611686930442781,\n",
       " 0.016094531212917478,\n",
       " 0.015967111449103993,\n",
       " 0.015652998593413223,\n",
       " 0.015039330489143951,\n",
       " 0.015183173034827612,\n",
       " 0.015362078171627363,\n",
       " 0.01503186254477888,\n",
       " 0.014504613706364395,\n",
       " 0.013898451674477551,\n",
       " 0.013885944970000421,\n",
       " 0.014177023409714226,\n",
       " 0.014177169336219042,\n",
       " 0.013780270251485227,\n",
       " 0.01348168196182101,\n",
       " 0.013417174916296695,\n",
       " 0.013299307481340283,\n",
       " 0.01297376629288233,\n",
       " 0.012723440717124434,\n",
       " 0.012930715169498699,\n",
       " 0.012838418556847192,\n",
       " 0.012526409244436645,\n",
       " 0.01229732919877591,\n",
       " 0.012611799203772976,\n",
       " 0.01253098406911078,\n",
       " 0.012411141848205185,\n",
       " 0.01205212535772775,\n",
       " 0.011629588949404985,\n",
       " 0.011520364565531243,\n",
       " 0.0118263994141888,\n",
       " 0.012300062871194667,\n",
       " 0.011778876147275926,\n",
       " 0.01142608846435091,\n",
       " 0.011367690199755594,\n",
       " 0.01167673138803566,\n",
       " 0.011563361661763265,\n",
       " 0.01129870516085245,\n",
       " 0.01077284977258245,\n",
       " 0.010919212302141382,\n",
       " 0.010955795989885673,\n",
       " 0.01096163102691699,\n",
       " 0.011162390658279618,\n",
       " 0.011218775778798866,\n",
       " 0.01108164611126347,\n",
       " 0.01076033605151683,\n",
       " 0.010404774138848457,\n",
       " 0.010286191034734344,\n",
       " 0.01044682689839737,\n",
       " 0.01064833357278252,\n",
       " 0.010782189283386734,\n",
       " 0.010330772843299264,\n",
       " 0.010291602205884072,\n",
       " 0.010224616993527594,\n",
       " 0.010378478783154803,\n",
       " 0.010322709173557085,\n",
       " 0.010109868785074937,\n",
       " 0.01003481099252105,\n",
       " 0.010240137413159569,\n",
       " 0.009959800366147388,\n",
       " 0.009881682955919928,\n",
       " 0.009871162164167368,\n",
       " 0.010198877363793024,\n",
       " 0.010298711124865585,\n",
       " 0.010001292621547189,\n",
       " 0.009830668066795055,\n",
       " 0.009518872107008578,\n",
       " 0.009763749642476116,\n",
       " 0.009957750166800118,\n",
       " 0.009863691452524487,\n",
       " 0.009734807427945798,\n",
       " 0.009583028250733807,\n",
       " 0.009723434056453508,\n",
       " 0.00951602870264783,\n",
       " 0.009547643186995927,\n",
       " 0.009506213413189493,\n",
       " 0.009542721390806246,\n",
       " 0.009584501435264679,\n",
       " 387.7521216869354]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"results/ECFP_Dipole_4_1024\", \"rb\") as f:\n",
    "    result = pickle.load(f)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6667, dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(torch.tensor((1,2,3),dtype=float),torch.tensor((1,4,4),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/pytorch/GCN/PyG.ipynb セル 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m math\u001b[39m.\u001b[39msqrt(result[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "\n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)\n",
    "\n",
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/pytorch/GCN/PyG.ipynb セル 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./qm9_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQM9_ECFP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "\n",
    "#ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "with open(\"QM9_ECFP\", \"rb\") as f:\n",
    "    ECFP = pickle.load(f)\n",
    "# 説明変数と説明変数\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values # 双極子モーメント\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 2\n",
    "nBits = 2048\n",
    "ECFP = [list(ECFPGen(smiles, radius=radius, nBits=nBits)) for smiles in df[\"smiles\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 581.53706374\n",
      "Validation score: -0.247182\n",
      "Iteration 2, loss = 32.07979582\n",
      "Validation score: 0.205536\n",
      "Iteration 3, loss = 24.30612155\n",
      "Validation score: 0.301358\n",
      "Iteration 4, loss = 21.94888746\n",
      "Validation score: 0.355603\n",
      "Iteration 5, loss = 20.29943940\n",
      "Validation score: 0.401968\n",
      "Iteration 6, loss = 18.72326213\n",
      "Validation score: 0.457337\n",
      "Iteration 7, loss = 16.21661796\n",
      "Validation score: 0.552198\n",
      "Iteration 8, loss = 12.70165132\n",
      "Validation score: 0.649496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/higuchi/mambaforge/envs/torchenv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "for \n",
    "hidden_layer_sizes = (100,)\n",
    "mlp = MLPRegressor(max_iter=10000, activation=\"relu\", solver=\"adam\", verbose=True, hidden_layer_sizes=hidden_layer_sizes, early_stopping=True)\n",
    "mlp.loss = \"squared_error\"\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values #alpha:分極率 idx=1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "r2 = []\n",
    "start = time.time()\n",
    "mlp.fit(X_train,Y_train)\n",
    "end = time.time()\n",
    "time_diff = end - start\n",
    "with open(f\"results/ECFP_alpha_{radius}_{nBits}_{hidden_layer_sizes}\", \"wb\") as f:\n",
    "    rmse = [math.sqrt(i*2) for i in mlp.loss_curve_] #scikit-learnのMSEは2で割られているので、2をかけてから平方根を取る。\n",
    "    result = [rmse, mlp.validation_scores_, time_diff]\n",
    "    pickle.dump(mlp.loss_curve_, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x154ec2939ed0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#仮word用\n",
    "with open(\"results/ECFP_alpha_2_2048_(100,)\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.24718238183448138,\n",
       " 0.20553624723876562,\n",
       " 0.3013577295413743,\n",
       " 0.35560322143194345,\n",
       " 0.40196792394190206,\n",
       " 0.4573367435465394,\n",
       " 0.5521981531675463,\n",
       " 0.6494963544839556]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.validation_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x154ec1715bd0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.332260841125844,\n",
       " 9.253041037596455,\n",
       " 7.774542167845601,\n",
       " 7.22113449508434,\n",
       " 6.925257883885077,\n",
       " 6.574595459266994,\n",
       " 6.185338698514044,\n",
       " 5.7203862932563885,\n",
       " 5.227872203251129,\n",
       " 4.755588329112674,\n",
       " 4.380516226115726,\n",
       " 4.118782087140771,\n",
       " 3.9376012997801464,\n",
       " 3.8052510160807365,\n",
       " 3.7097364731547775,\n",
       " 3.6386088550162996,\n",
       " 3.5805889225188934,\n",
       " 3.5320462827930923,\n",
       " 3.493693867969131,\n",
       " 3.4593303722255144,\n",
       " 3.427472758266009,\n",
       " 3.3993649428717867,\n",
       " 3.377161120583056,\n",
       " 3.355387808196992,\n",
       " 3.334563602068595,\n",
       " 3.316529344610288,\n",
       " 3.3004407360407715,\n",
       " 3.2871856446872068,\n",
       " 3.271102767883358,\n",
       " 3.259451253180907,\n",
       " 3.2480761367015383,\n",
       " 3.235460094345634,\n",
       " 3.221325572043916,\n",
       " 3.2115736949874694,\n",
       " 3.200076339710851,\n",
       " 3.194678485969184,\n",
       " 3.184324915302172,\n",
       " 3.1754030434302423,\n",
       " 3.1648268042140995,\n",
       " 3.1584227481047344,\n",
       " 3.1532293746578333,\n",
       " 3.1473948891649717,\n",
       " 3.138079039227738,\n",
       " 3.1298950468495113,\n",
       " 3.129959481678678,\n",
       " 3.120580528972171,\n",
       " 3.1157564462239935,\n",
       " 3.1105329080210056,\n",
       " 3.104473504414136,\n",
       " 3.097308459531519,\n",
       " 3.0940017435966047,\n",
       " 3.091448324894799,\n",
       " 3.0837665670704473,\n",
       " 3.079793029834387,\n",
       " 3.0762489895420715,\n",
       " 3.072965815432913,\n",
       " 3.066746512635955,\n",
       " 3.0633727278901883,\n",
       " 3.0616900139184713,\n",
       " 3.0558252603267118,\n",
       " 3.0527395236324075,\n",
       " 3.048983440541145,\n",
       " 3.045592992488555,\n",
       " 3.0416500418750516,\n",
       " 3.038110376516928,\n",
       " 3.0354006942325045,\n",
       " 3.033999527606488,\n",
       " 3.0300667654461035,\n",
       " 3.026513171192985,\n",
       " 3.0227494634016874,\n",
       " 3.0223701938852328,\n",
       " 3.0188930717739604,\n",
       " 3.017245057747787,\n",
       " 3.0138082769094696,\n",
       " 3.010626527915114,\n",
       " 3.0111471881193648,\n",
       " 3.0066992709509828,\n",
       " 3.0043435839734522,\n",
       " 3.001413517747523,\n",
       " 3.000185332074748,\n",
       " 2.9942901983715324,\n",
       " 2.9958539575827414,\n",
       " 2.991857931365636,\n",
       " 2.9896247447293853,\n",
       " 2.9876887024885654,\n",
       " 2.9860962756553975,\n",
       " 2.9884775185385823,\n",
       " 2.983685949872368,\n",
       " 2.9813370329816014,\n",
       " 2.9791789896842444,\n",
       " 2.977031406560667,\n",
       " 2.975484737677077,\n",
       " 2.9715359320363057,\n",
       " 2.975055139977957,\n",
       " 2.9707568191037894,\n",
       " 2.972995072572144,\n",
       " 2.9682896853437044,\n",
       " 2.9664878332094604,\n",
       " 2.963744202963967,\n",
       " 2.9617115159509657,\n",
       " 2.9629354921609528,\n",
       " 2.9595303632838887,\n",
       " 2.9573983963332693,\n",
       " 2.9569553378118374,\n",
       " 2.9543438987733706,\n",
       " 2.9553025980362446,\n",
       " 2.9537679184201715,\n",
       " 2.9482390935422447,\n",
       " 2.948597716984095,\n",
       " 2.947497081111831,\n",
       " 2.9456706939109574,\n",
       " 2.9456656622182398,\n",
       " 2.9438627667156014,\n",
       " 2.9415507715244753,\n",
       " 2.9402177069279003,\n",
       " 2.9401436555819345,\n",
       " 2.9396908883195607,\n",
       " 2.938293789162118,\n",
       " 2.9363293787906137,\n",
       " 2.936064838654833,\n",
       " 2.935548311470701,\n",
       " 2.9326675517086844,\n",
       " 2.931038001536504,\n",
       " 2.929757186393099,\n",
       " 2.92910053055926,\n",
       " 2.9308331846505085,\n",
       " 2.9299230145754875,\n",
       " 2.9263232355495816,\n",
       " 2.925078975275345,\n",
       " 2.9232344606578557,\n",
       " 2.920576097667605,\n",
       " 2.920913459890696,\n",
       " 2.9227167038455106,\n",
       " 2.917319578142843,\n",
       " 2.918625321259293,\n",
       " 2.9186139713417165,\n",
       " 2.9166917437548894,\n",
       " 2.9186502928812197,\n",
       " 2.9151927653826317,\n",
       " 2.9130910923562943,\n",
       " 2.9165758318536508,\n",
       " 2.909457372498613,\n",
       " 2.909354753834894,\n",
       " 2.90836623785344,\n",
       " 2.9075542666443637,\n",
       " 2.9107075046894866,\n",
       " 2.9083690631877315,\n",
       " 2.9051131915671866,\n",
       " 2.902706804893382,\n",
       " 2.904067846261138,\n",
       " 2.9027235977015406,\n",
       " 2.903192762730738,\n",
       " 2.903045514329991,\n",
       " 2.901006464873283,\n",
       " 2.895814196552968,\n",
       " 2.9013094094581184,\n",
       " 2.896099984713804,\n",
       " 2.898447429826928,\n",
       " 2.896298422226547,\n",
       " 2.8953202610692195,\n",
       " 2.895594917011016,\n",
       " 2.892697090616718,\n",
       " 2.8921937858767555,\n",
       " 2.8928643967439407,\n",
       " 2.893183365348515,\n",
       " 2.8908224768358317,\n",
       " 2.890301189634693,\n",
       " 2.891834563889866,\n",
       " 2.8897437546053717,\n",
       " 2.888837625778588,\n",
       " 2.885341932190685,\n",
       " 2.88454815142888,\n",
       " 2.8870677501513766,\n",
       " 2.8874901926232397,\n",
       " 2.885338052274565,\n",
       " 2.8859099599926292,\n",
       " 2.8834885554087055,\n",
       " 2.8819922868938463,\n",
       " 2.880614161914589,\n",
       " 2.884170233771983,\n",
       " 2.8800010783524486,\n",
       " 2.8790509700321,\n",
       " 2.8812163439926617,\n",
       " 2.8781910043262733,\n",
       " 2.877654231203316,\n",
       " 2.8775838284358404,\n",
       " 2.8741464882361902,\n",
       " 2.8755784951208474,\n",
       " 2.875075089436577,\n",
       " 2.873205628739723,\n",
       " 2.8734041257882166,\n",
       " 2.8722608141698673,\n",
       " 2.877194206256834,\n",
       " 2.870928900578609,\n",
       " 2.87214629050634,\n",
       " 2.873910178402441,\n",
       " 2.870517541366565,\n",
       " 2.868699669424772,\n",
       " 2.870342528102558,\n",
       " 2.867894618117668,\n",
       " 2.8683190071509648,\n",
       " 2.868444654444253,\n",
       " 2.8692853441269692,\n",
       " 2.8659610157089066,\n",
       " 2.866838407476268,\n",
       " 2.8654329166473937,\n",
       " 2.8649220809698988,\n",
       " 2.8689379910424337,\n",
       " 2.864437957540471,\n",
       " 2.862845650429788,\n",
       " 2.862952250059964,\n",
       " 2.8635016298857314,\n",
       " 2.860330290788684,\n",
       " 2.8597693751963646,\n",
       " 2.8611788956134543,\n",
       " 2.859725835740209,\n",
       " 2.860247989712367,\n",
       " 2.859271987443499,\n",
       " 2.8576860213180333,\n",
       " 2.8560223545205474,\n",
       " 2.858912927546576,\n",
       " 2.8571646539793267,\n",
       " 2.855693729818242,\n",
       " 2.8535191843042536,\n",
       " 2.8533958665291217,\n",
       " 2.8547360567224294,\n",
       " 2.855299151851945,\n",
       " 2.8561302937120736,\n",
       " 2.850213354339159,\n",
       " 2.852166688493105,\n",
       " 2.8529376354645346,\n",
       " 2.8513753097563117,\n",
       " 2.849253551307779,\n",
       " 2.850128927364497,\n",
       " 2.850885820933675,\n",
       " 2.8511297036741006,\n",
       " 2.8474091486656894,\n",
       " 2.849074424779193,\n",
       " 2.850000156080564,\n",
       " 2.8472639757817815,\n",
       " 2.847782547146107,\n",
       " 2.846426499571507,\n",
       " 2.849474613350106,\n",
       " 2.8461716683898217,\n",
       " 2.844596167097291,\n",
       " 2.8455169617273115,\n",
       " 2.844351522713813,\n",
       " 2.8432937347716445,\n",
       " 2.8419404049477976,\n",
       " 2.8436523727771417,\n",
       " 2.8410718310611927,\n",
       " 2.843557225645587,\n",
       " 2.84290178911583,\n",
       " 2.840359228546355,\n",
       " 2.8401146942174296,\n",
       " 2.8418608060421273,\n",
       " 2.840436758717803,\n",
       " 2.8369713709756423,\n",
       " 2.83725868745991,\n",
       " 2.838280965815894,\n",
       " 2.836225723325902,\n",
       " 2.8359858165028684,\n",
       " 2.8347306192457515,\n",
       " 2.837401096654494,\n",
       " 2.8357239368385985,\n",
       " 2.8351187531715207,\n",
       " 2.8344911683968577,\n",
       " 2.832900218884,\n",
       " 2.8351657384384743,\n",
       " 2.8336448885551286,\n",
       " 2.833251014559636,\n",
       " 2.8330229282659585,\n",
       " 2.833994456689604,\n",
       " 2.8333577541682686,\n",
       " 2.829345408170302,\n",
       " 2.8309661367369996,\n",
       " 2.8308415095297867,\n",
       " 2.832589138813185,\n",
       " 2.829213134324984,\n",
       " 2.831232396411808,\n",
       " 2.8286033857671358,\n",
       " 2.8304486859637805,\n",
       " 2.830008113116399,\n",
       " 2.8272613323123363,\n",
       " 2.830070563290161,\n",
       " 2.827843810282796,\n",
       " 2.827836615555355,\n",
       " 2.8258873998272627,\n",
       " 2.8286286714308524,\n",
       " 2.82604397326836,\n",
       " 2.8243417173314325,\n",
       " 2.8234011361238562,\n",
       " 2.822498942196007,\n",
       " 2.823642501657429,\n",
       " 2.8239916962840272,\n",
       " 2.8244877942340474,\n",
       " 2.822116510080853,\n",
       " 2.8232157175841737,\n",
       " 2.820206807707949,\n",
       " 2.821644303157668,\n",
       " 2.820795905645793,\n",
       " 2.8218579060919002,\n",
       " 2.8211867301255724,\n",
       " 2.8196926928084984,\n",
       " 2.819788190596959,\n",
       " 2.819780639933113,\n",
       " 2.8206794241362143,\n",
       " 2.817468385696387,\n",
       " 2.8183788708051147,\n",
       " 2.8155808747357196,\n",
       " 2.81973034933021,\n",
       " 2.8179076947875976,\n",
       " 2.816745535503068,\n",
       " 2.8172990227197636,\n",
       " 2.8148247314983372,\n",
       " 2.8175997119258183,\n",
       " 2.8141388481883793,\n",
       " 2.8136861465885445,\n",
       " 2.814929392458323,\n",
       " 2.8162641558771724,\n",
       " 2.8118193581738637,\n",
       " 2.8140561236154737,\n",
       " 2.8160220810719974,\n",
       " 2.812153510948009,\n",
       " 2.81117485803084,\n",
       " 2.8103279807015253,\n",
       " 2.810736103702643,\n",
       " 2.8130515848714577,\n",
       " 2.8111449594806683,\n",
       " 2.8137949935358715,\n",
       " 2.8092501182378062,\n",
       " 2.8108743311882667,\n",
       " 2.8117848984261657,\n",
       " 2.808544174074407,\n",
       " 2.8088543175371536,\n",
       " 2.8103801578749636,\n",
       " 2.8105274863645633,\n",
       " 2.8064813358192433,\n",
       " 2.807089062806461,\n",
       " 2.8101392001275043,\n",
       " 2.8064401176942795,\n",
       " 2.8057579005649553,\n",
       " 2.8074707595027455,\n",
       " 2.8044891093881223,\n",
       " 2.806448404425655,\n",
       " 2.804878210030613,\n",
       " 2.8048121055371342,\n",
       " 2.8052659888207208,\n",
       " 2.8045847240147235,\n",
       " 2.803706836477716,\n",
       " 2.8036077075960115,\n",
       " 2.8024714511566016,\n",
       " 2.803166148657326,\n",
       " 2.8035625738554804,\n",
       " 2.804299797202602,\n",
       " 2.8002388988843006,\n",
       " 2.8020694458890847,\n",
       " 2.8024636559749347,\n",
       " 2.800718686931896,\n",
       " 2.8010873969708854,\n",
       " 2.8016951574097924,\n",
       " 2.800800108854816,\n",
       " 2.799167412168519,\n",
       " 2.800405154282001,\n",
       " 2.799943931538362,\n",
       " 2.798851571731364,\n",
       " 2.7989790879493155,\n",
       " 2.7984712779038303,\n",
       " 2.7960771692084383,\n",
       " 2.7970558688228344,\n",
       " 2.797171126169726,\n",
       " 2.796366300913365,\n",
       " 2.7975618565291946,\n",
       " 2.797760783936664,\n",
       " 2.7957033046912296,\n",
       " 2.794661491296401,\n",
       " 2.7949594671633973,\n",
       " 2.7953946966139767,\n",
       " 2.7965293704260716,\n",
       " 2.7959153807061954,\n",
       " 2.7917065076100154,\n",
       " 2.7941433586743876,\n",
       " 2.7926916456283744,\n",
       " 2.7939797840417095,\n",
       " 2.7898755689316364,\n",
       " 2.7904593335844443,\n",
       " 2.792664877662353,\n",
       " 2.7935021997569702,\n",
       " 2.790177052066664,\n",
       " 2.7926005941543433,\n",
       " 2.789649106312909,\n",
       " 2.7926448045154655,\n",
       " 2.790361490076341,\n",
       " 2.7908153540663756,\n",
       " 2.791356347594224,\n",
       " 2.7890469674221947,\n",
       " 2.788338295611843,\n",
       " 2.787847706871971,\n",
       " 2.789652918038037,\n",
       " 2.7890889374182106,\n",
       " 2.788527344654226,\n",
       " 2.790115635420084,\n",
       " 2.7880107069142652,\n",
       " 2.7885785692491845,\n",
       " 2.789462961662163,\n",
       " 2.7875452911558956,\n",
       " 2.786362739202203,\n",
       " 2.7862846299698223,\n",
       " 2.787372669379492,\n",
       " 2.7854437079706487,\n",
       " 2.787136052422316,\n",
       " 2.783199386858608,\n",
       " 2.784540957526583,\n",
       " 2.7852883289982158,\n",
       " 2.784848470069861,\n",
       " 2.7856331673630286,\n",
       " 2.7820915909581374,\n",
       " 2.7814324326902464,\n",
       " 2.782924277739855,\n",
       " 2.783330478809298,\n",
       " 2.783783558081631,\n",
       " 2.782286692820479,\n",
       " 2.7833452998226655,\n",
       " 2.7827520904632626,\n",
       " 2.7815924667719036,\n",
       " 2.780418827669595,\n",
       " 2.7797834740023735,\n",
       " 2.7809800554106574,\n",
       " 2.781071289831565,\n",
       " 2.7816725458406335,\n",
       " 2.781712528470211,\n",
       " 2.7801388087915644,\n",
       " 2.7780438169297157,\n",
       " 2.778782751715509,\n",
       " 2.781764672846426,\n",
       " 2.7782887060674613,\n",
       " 2.77810701879162,\n",
       " 2.778977624740111,\n",
       " 2.7776828483795,\n",
       " 2.7793035608621697,\n",
       " 2.777214912822968,\n",
       " 2.7764348871872975,\n",
       " 2.777409343874555,\n",
       " 2.77645627522426,\n",
       " 2.775970775298527,\n",
       " 2.7761159165663023,\n",
       " 2.7773566352855914,\n",
       " 2.7772614560632105,\n",
       " 2.7728445100703016,\n",
       " 2.7752699310884656,\n",
       " 2.7739831531948713,\n",
       " 2.7752673846907907,\n",
       " 2.776190628048404,\n",
       " 2.774561841508254,\n",
       " 2.7724989291400064,\n",
       " 2.7737454200691123,\n",
       " 2.7753511409753173,\n",
       " 2.773527816683009,\n",
       " 2.7717767471129378,\n",
       " 2.775577118543453,\n",
       " 2.772378951787522,\n",
       " 2.7712307306316215,\n",
       " 2.773551762510349,\n",
       " 2.7720741236872417,\n",
       " 2.769554973659614,\n",
       " 2.7719362430325725,\n",
       " 2.771320655766888,\n",
       " 2.7698723517713915,\n",
       " 2.769828665280646,\n",
       " 2.771644722777531,\n",
       " 2.771370525226267,\n",
       " 2.7680959978927273,\n",
       " 2.7727327701343842,\n",
       " 2.7686376597436415,\n",
       " 2.771714260957411,\n",
       " 2.76821675115692,\n",
       " 2.7691786841390638,\n",
       " 2.7671853909013815,\n",
       " 2.768432572312823,\n",
       " 2.7675397980697447,\n",
       " 2.767691475361848,\n",
       " 2.7705132491810693,\n",
       " 2.7675003035023655,\n",
       " 2.768498873106922,\n",
       " 2.7655682484062734,\n",
       " 2.7665932347109594,\n",
       " 2.7682137440589756,\n",
       " 2.767658528702778,\n",
       " 2.7665034862373976,\n",
       " 2.764895266168437,\n",
       " 2.765194263730068,\n",
       " 2.7657944828361902,\n",
       " 2.763742431521539,\n",
       " 2.766468302503608,\n",
       " 2.7655372052360274,\n",
       " 2.7630889385346395,\n",
       " 2.764223025755403,\n",
       " 2.7643719351395455,\n",
       " 2.7641771138093096,\n",
       " 2.764375425646467,\n",
       " 2.7616659469100053,\n",
       " 2.762074735368168,\n",
       " 2.763935643732205,\n",
       " 2.7637170994940172,\n",
       " 2.761940164337272,\n",
       " 2.7632914624000398,\n",
       " 2.7608572061255594,\n",
       " 2.7644730826982014,\n",
       " 2.76151382695052,\n",
       " 2.7639161344549823,\n",
       " 2.759894545238893,\n",
       " 2.762255156012993,\n",
       " 2.7620221771047864,\n",
       " 2.759972682346499,\n",
       " 2.759626105928244,\n",
       " 2.761143403415411,\n",
       " 2.760991658162546,\n",
       " 2.7611403902742797,\n",
       " 2.758714582329008,\n",
       " 2.7598918089983617,\n",
       " 2.7613343130147427,\n",
       " 2.7598934251438694,\n",
       " 2.7601910909503244,\n",
       " 2.7589009067007493,\n",
       " 2.758724352812956,\n",
       " 2.758492547997235,\n",
       " 2.758582970927466,\n",
       " 2.758979523441346,\n",
       " 2.758416681448794,\n",
       " 2.757879576848751,\n",
       " 2.7578435304227233,\n",
       " 2.7569662822213705,\n",
       " 2.7601042513924403,\n",
       " 2.7561321763247038,\n",
       " 2.7563134854538336,\n",
       " 2.759559973325579,\n",
       " 2.755550838339293,\n",
       " 2.7583744372676438,\n",
       " 2.757574837163975,\n",
       " 2.7555574973118677,\n",
       " 2.7574546416669423,\n",
       " 2.7562292568632323,\n",
       " 2.756026701151644,\n",
       " 2.75697464767131,\n",
       " 2.7547904185903707,\n",
       " 2.7529200205066764,\n",
       " 2.7534263616542516,\n",
       " 2.754139312945925,\n",
       " 2.755401218425426,\n",
       " 2.752390757852816,\n",
       " 2.7540905755440965,\n",
       " 2.754583427085757,\n",
       " 2.752503093758506,\n",
       " 2.754283244493616,\n",
       " 2.753539103858805,\n",
       " 2.7530051361686168,\n",
       " 2.752492309690032,\n",
       " 2.751625793742965,\n",
       " 2.7530296240271257,\n",
       " 2.75260808681791,\n",
       " 2.7542497058894675,\n",
       " 2.7522059259723295,\n",
       " 2.7528528978696825,\n",
       " 2.753193945994677,\n",
       " 2.750486167954428,\n",
       " 2.7509112380240532,\n",
       " 2.750921587857897,\n",
       " 2.7528584292979215,\n",
       " 2.7515627937291156,\n",
       " 2.7504110219077846,\n",
       " 2.749785599184345,\n",
       " 2.7505711997874815,\n",
       " 2.75234302028661,\n",
       " 2.749440436897837,\n",
       " 2.750911977791026,\n",
       " 2.747583682502563,\n",
       " 2.7536129134423217,\n",
       " 2.7484373623935383,\n",
       " 2.7496914385426767,\n",
       " 2.750590075258902,\n",
       " 2.7467400788301144,\n",
       " 2.7492077231865455,\n",
       " 2.7506290271078297,\n",
       " 2.7481907976615507,\n",
       " 2.750796512668881,\n",
       " 2.7476163508708087,\n",
       " 2.7498206069298474,\n",
       " 2.749754816160542,\n",
       " 2.7470471296738834,\n",
       " 2.748316077338323,\n",
       " 2.74521194660853,\n",
       " 2.746632484903623,\n",
       " 2.747050178629075,\n",
       " 2.745189584190284,\n",
       " 2.746780323250074,\n",
       " 2.746932339281232,\n",
       " 2.746962913207523,\n",
       " 2.743380975575094,\n",
       " 2.7460468219734238,\n",
       " 2.744838272026058,\n",
       " 2.7468689609486963,\n",
       " 2.7459738895445587,\n",
       " 2.7443232837282863,\n",
       " 2.7435206411805777,\n",
       " 2.743405143416411,\n",
       " 2.7430804111067517,\n",
       " 2.7453990319677404,\n",
       " 2.7439822228819413,\n",
       " 2.744885617705269,\n",
       " 2.7446250625914925,\n",
       " 2.7426337275488226,\n",
       " 2.7419311435809584,\n",
       " 2.7438116081753656,\n",
       " 2.742946147829299,\n",
       " 2.742869440443662,\n",
       " 2.7433698540107487,\n",
       " 2.7442988347655497,\n",
       " 2.7421943426802327,\n",
       " 2.744423838270662,\n",
       " 2.744059139927731,\n",
       " 2.740984261984596,\n",
       " 2.7403424614889866,\n",
       " 2.743052464745129,\n",
       " 2.744360793571922,\n",
       " 2.7392249726700677,\n",
       " 2.740118962265614,\n",
       " 2.7413097010013288,\n",
       " 2.741770702874555,\n",
       " 2.7396600578044357,\n",
       " 2.740561232226468,\n",
       " 2.7404620525988808,\n",
       " 2.743564734206806,\n",
       " 2.738820757545563,\n",
       " 2.7422423728042946,\n",
       " 2.7405173752146497,\n",
       " 2.7420964977955555,\n",
       " 2.740367260844425,\n",
       " 2.7390183029880735,\n",
       " 2.7388866737789135,\n",
       " 2.740775253710101,\n",
       " 2.7373174154438846,\n",
       " 2.7397049862825336,\n",
       " 2.7423710001606554,\n",
       " 2.738339860997504,\n",
       " 2.7395596567478857,\n",
       " 2.73918405484439,\n",
       " 2.7406611949117554,\n",
       " 2.73663846868383,\n",
       " 2.737720112973781,\n",
       " 2.7378594888581196,\n",
       " 2.736989345915717,\n",
       " 2.737927768782956,\n",
       " 2.7380573866074185,\n",
       " 2.7362307900122627,\n",
       " 2.7344310425936023,\n",
       " 2.7380410035011393,\n",
       " 2.7383497433173587,\n",
       " 831.1410341262817]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'squared_error'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(max_iter=1000, activation=\"relu\", solver=\"adam\", verbose=True, hidden_layer_sizes=hidden_layer_sizes)\n",
    "mlp.loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7526752245422885"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RegressorMixin.score() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/pytorch/GCN/PyG.ipynb セル 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mlp\u001b[39m.\u001b[39;49mscore()\n",
      "\u001b[0;31mTypeError\u001b[0m: RegressorMixin.score() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "mlp.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.count() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/higuchi/pytorch/GCN/PyG.ipynb セル 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B133.28.55.128/home/higuchi/pytorch/GCN/PyG.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ECFP\u001b[39m.\u001b[39;49mcount()\n",
      "\u001b[0;31mTypeError\u001b[0m: list.count() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(max_iter=10000, activation=\"relu\", solver=\"adam\", verbose=True, hidden_layer_sizes=(100,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/ECFP_Dipole_2_22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7358804574041745,\n",
       " 0.5027598216023492,\n",
       " 0.4048464232582225,\n",
       " 0.32131983580670886,\n",
       " 0.2571448085987245,\n",
       " 0.2082152817909352,\n",
       " 0.17140689485777466,\n",
       " 0.14493100514163731,\n",
       " 0.12429132858163466,\n",
       " 0.10790171045920356,\n",
       " 0.09508175045970438,\n",
       " 0.08503357051125295,\n",
       " 0.07647326190441184,\n",
       " 0.06908736190112823,\n",
       " 0.06389540771923595,\n",
       " 0.05890218367973595,\n",
       " 0.054587610416104715,\n",
       " 0.05173371211173319,\n",
       " 0.048063734165877225,\n",
       " 0.04511863307380085,\n",
       " 0.04318921424900014,\n",
       " 0.04124397413974403,\n",
       " 0.03910365244625134,\n",
       " 0.03720496465669037,\n",
       " 0.03603510861041594,\n",
       " 0.034676303454308,\n",
       " 0.033155915144484766,\n",
       " 0.03204129150098049,\n",
       " 0.0304936250658086,\n",
       " 0.029531188950549733,\n",
       " 0.028670449175493544,\n",
       " 0.02734130630772425,\n",
       " 0.026207421230787623,\n",
       " 0.026111303579746777,\n",
       " 0.025301557045952078,\n",
       " 0.02413884025673116,\n",
       " 0.023880908083979448,\n",
       " 0.023046258473110773,\n",
       " 0.02210988478668418,\n",
       " 0.02176290456014009,\n",
       " 0.021210568614976586,\n",
       " 0.020716124321804728,\n",
       " 0.02007628697400676,\n",
       " 0.01973294985104709,\n",
       " 0.01939769913228593,\n",
       " 0.018983260196333188,\n",
       " 0.018529435906940794,\n",
       " 0.018434618372327863,\n",
       " 0.018043997826468046,\n",
       " 0.017600647943415034,\n",
       " 0.016816782693121306,\n",
       " 0.016939506071407814,\n",
       " 0.016705210698075592,\n",
       " 0.016987033930138527,\n",
       " 0.016281062049112564,\n",
       " 0.015352382555076776,\n",
       " 0.015328020623416391,\n",
       " 0.015200708967433646,\n",
       " 0.01533062197877878,\n",
       " 0.01549585904607294,\n",
       " 0.01526807004903967,\n",
       " 0.014671757519123281,\n",
       " 0.014472570484280954,\n",
       " 0.014431377031827623,\n",
       " 0.01427831253604732,\n",
       " 0.013653374286883272,\n",
       " 0.013303272694865217,\n",
       " 0.013898035408534255,\n",
       " 0.013749494281242189,\n",
       " 0.013916757994102047,\n",
       " 0.013426618025372233,\n",
       " 0.013121336815781575,\n",
       " 0.012808811068927983,\n",
       " 0.012751863846425753,\n",
       " 0.012937614033133026,\n",
       " 0.012918840733537152,\n",
       " 0.012742673816053353,\n",
       " 0.01242990868044378,\n",
       " 0.012221744665828515,\n",
       " 0.012083233373092011,\n",
       " 0.011976093266206543,\n",
       " 0.011964559466673125,\n",
       " 0.012168868432215412,\n",
       " 0.011985273688528197,\n",
       " 0.012034774760814463,\n",
       " 0.011830183940926749,\n",
       " 0.011833323541441131,\n",
       " 0.011569598438823717,\n",
       " 0.011256865221440817,\n",
       " 0.011205435973350714,\n",
       " 0.01111388511761013,\n",
       " 0.011304557434434669,\n",
       " 0.011641061504212366,\n",
       " 0.01138842951237596,\n",
       " 0.011271930741134698,\n",
       " 0.01089267734138016,\n",
       " 0.01071330155700205,\n",
       " 0.010837809097848827,\n",
       " 0.011169495883713427,\n",
       " 0.011011839285297371,\n",
       " 0.010687013705017895,\n",
       " 0.010617367837559118,\n",
       " 0.010312131680897476,\n",
       " 0.010379990120427576,\n",
       " 0.010684554320258817,\n",
       " 0.01077987597308878,\n",
       " 0.010442640827734388,\n",
       " 0.010176295839268669,\n",
       " 0.01022643095526178,\n",
       " 0.010261026425864411,\n",
       " 0.010236436817401032,\n",
       " 0.010363901463118537,\n",
       " 0.01004290521014762,\n",
       " 0.009841998420743278,\n",
       " 0.009771456363151888,\n",
       " 0.01004357507279061,\n",
       " 0.010040248483107148,\n",
       " 0.010256952170218677,\n",
       " 0.009985585165953248,\n",
       " 0.010004430477477722,\n",
       " 0.009936302357040834,\n",
       " 0.009706989718931873,\n",
       " 0.009585133429353425,\n",
       " 0.009614676068177787,\n",
       " 0.009580179641702343,\n",
       " 0.009773374248170564,\n",
       " 0.00970885657719962,\n",
       " 0.00978311325301176,\n",
       " 0.0096478728120131,\n",
       " 0.009376017726869858,\n",
       " 0.009294432938768906,\n",
       " 0.009364938550979515,\n",
       " 0.00950119645347348,\n",
       " 0.00953953641696284,\n",
       " 0.009495968670232068,\n",
       " 0.009347220320469632,\n",
       " 0.009324215090856442,\n",
       " 0.009173145532222673,\n",
       " 0.009134709710628389,\n",
       " 0.009290688668964825,\n",
       " 0.009110728377395669,\n",
       " 0.009524084013026323,\n",
       " 0.009349305860351986,\n",
       " 0.009278602321949152,\n",
       " 0.009189356031507043,\n",
       " 0.008942082770357514,\n",
       " 0.008903824510761283,\n",
       " 0.00901524064437452,\n",
       " 0.009178984366822133,\n",
       " 0.009137975313844441,\n",
       " 0.008936748522734237,\n",
       " 0.008896008957417068,\n",
       " 0.00879250545178723,\n",
       " 0.008933480771102674,\n",
       " 0.009404199666783672,\n",
       " 0.009236127623480361,\n",
       " 0.008886555791973463,\n",
       " 0.0087419831880849,\n",
       " 0.008589701250810497,\n",
       " 0.008919320406043598,\n",
       " 0.009068526963334926,\n",
       " 0.009019872141547862,\n",
       " 0.008879315116807899,\n",
       " 0.008695255711824012,\n",
       " 0.008756893280321284,\n",
       " 0.00890049550558596,\n",
       " 0.008807845880983623,\n",
       " 0.008622340389678488,\n",
       " 0.008768751944776396,\n",
       " 0.008778067121215222,\n",
       " 652.8379423618317]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"results/ECFP_Dipole_6_2048\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7212349688603316,\n",
       " 0.4924714119535072,\n",
       " 0.3985999208704326,\n",
       " 0.31731232789205394,\n",
       " 0.252247227542993,\n",
       " 0.2034815230259134,\n",
       " 0.16854704065890996,\n",
       " 0.14268826020542905,\n",
       " 0.12250768413503137,\n",
       " 0.10625390320547484,\n",
       " 0.09488068803553656,\n",
       " 0.08432461336949365,\n",
       " 0.07610654692776328,\n",
       " 0.06927764140194327,\n",
       " 0.06300736611895061,\n",
       " 0.058749097281114236,\n",
       " 0.05426655092195708,\n",
       " 0.05046753198162567,\n",
       " 0.047528318433161465,\n",
       " 0.044585887511101416,\n",
       " 0.042295489663074934,\n",
       " 0.040110069159544855,\n",
       " 0.038093316894755896,\n",
       " 0.03629375489557763,\n",
       " 0.03464788015827229,\n",
       " 0.03326209625057759,\n",
       " 0.03187052817997083,\n",
       " 0.030942121430672632,\n",
       " 0.029797125495898157,\n",
       " 0.028921119050303607,\n",
       " 0.02801924034814353,\n",
       " 0.026479150507085693,\n",
       " 0.02586889376848087,\n",
       " 0.02561227483039702,\n",
       " 0.024699789604930047,\n",
       " 0.023967034268906083,\n",
       " 0.022989941841608096,\n",
       " 0.02219969095173206,\n",
       " 0.021866385209806816,\n",
       " 0.021160211230047618,\n",
       " 0.02094596889340653,\n",
       " 0.02014500279458145,\n",
       " 0.019044652336483853,\n",
       " 0.01891095064668913,\n",
       " 0.019095343659417657,\n",
       " 0.018562093272392503,\n",
       " 0.017565014312832365,\n",
       " 0.017402679340718492,\n",
       " 0.017315866566433837,\n",
       " 0.01687758278057574,\n",
       " 0.016433516023255445,\n",
       " 0.01635346818215752,\n",
       " 0.01600862155901457,\n",
       " 0.015622796844450204,\n",
       " 0.01540193909755386,\n",
       " 0.015221841539285887,\n",
       " 0.015220197390160785,\n",
       " 0.014924953948078317,\n",
       " 0.01475812065004116,\n",
       " 0.01443574359507679,\n",
       " 0.014094245283358285,\n",
       " 0.014080640156860348,\n",
       " 0.013983394327818737,\n",
       " 0.014033026678423217,\n",
       " 0.013549783704123988,\n",
       " 0.012997947235084463,\n",
       " 0.012742201276323101,\n",
       " 0.01329895228706027,\n",
       " 0.013485542322470791,\n",
       " 0.013223485135186801,\n",
       " 0.012757105409969887,\n",
       " 0.012424558059366594,\n",
       " 0.012310514714714865,\n",
       " 0.012302102471037902,\n",
       " 0.01232499153027579,\n",
       " 0.012413686711531735,\n",
       " 0.012255166723969334,\n",
       " 0.011920386121764466,\n",
       " 0.012091800770499962,\n",
       " 0.011879707436483183,\n",
       " 0.011628272926996074,\n",
       " 0.01146528288245688,\n",
       " 0.011507339866087634,\n",
       " 0.011376258367916055,\n",
       " 0.011494478229438053,\n",
       " 0.011212666587648065,\n",
       " 0.011126395142791167,\n",
       " 0.011119059098622057,\n",
       " 0.011160932195305328,\n",
       " 0.01096503528810338,\n",
       " 0.011026864691135873,\n",
       " 0.010907371369208486,\n",
       " 0.010939351948921407,\n",
       " 0.010782281625879828,\n",
       " 0.010672811483844859,\n",
       " 0.010627629310758236,\n",
       " 0.010407618136692236,\n",
       " 0.010386740800993634,\n",
       " 0.010433978297766572,\n",
       " 0.010547597888189432,\n",
       " 0.010558013509876668,\n",
       " 0.010240597990716312,\n",
       " 0.009946538207469375,\n",
       " 0.009977468865630501,\n",
       " 0.010314883789728203,\n",
       " 0.010306131722951709,\n",
       " 0.01008912636117857,\n",
       " 0.010101954000014792,\n",
       " 0.009980196553980563,\n",
       " 0.00997605464299039,\n",
       " 0.00997160795462326,\n",
       " 0.009664992208544788,\n",
       " 0.00968524053951858,\n",
       " 0.009809596604944504,\n",
       " 0.009835891906547873,\n",
       " 0.009748480929453933,\n",
       " 0.009715146132091602,\n",
       " 0.009484956554910166,\n",
       " 0.009499675392918553,\n",
       " 0.009540785976754139,\n",
       " 0.009736014504348176,\n",
       " 0.009741042231120467,\n",
       " 0.009389086335497732,\n",
       " 0.009179008501360063,\n",
       " 0.00934959788730982,\n",
       " 0.009482609645246042,\n",
       " 0.009446627032984989,\n",
       " 0.009412518286982172,\n",
       " 0.009299573650359539,\n",
       " 0.009233358148573173,\n",
       " 0.009087690111920869,\n",
       " 0.008993531481194885,\n",
       " 0.009149454498577271,\n",
       " 0.00952751149473462,\n",
       " 0.009243399215467782]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp.loss_curve_[:500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=64, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "error_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80] \n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "list(df[\"smiles\"])\n",
    "atomrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyFirstDataset(root=\"MyFirstDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, i in df.iterrows():\n",
    "    target = [float(x) for x in i.values[2:]]\n",
    "    target = torch.tensor(target, dtype=torch.float)\n",
    "    #target = torch.cat([target[:, 3:], target[:, 3:]], dim=0)\n",
    "    #target = target * conversion.view(1, -1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32, dataset=dataset):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dataset = dataset\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(self.dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(1, self.layer):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GAT_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATConv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATConv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class trans_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index, edge_attr[0])\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index, edge_attr[0])\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GNNModel_N(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.8), int(len(dataset)*0.1)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "#layer = 2\n",
    "#dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "Epoch 1 | train_loss:3.799780875091046, valid_loss:2.1569826146665734\n",
      "Epoch 2 | train_loss:2.4339145714010018, valid_loss:2.208805357861647\n",
      "Epoch 3 | train_loss:2.2565753938634185, valid_loss:2.0834259701087148\n",
      "Epoch 4 | train_loss:2.0575890960778933, valid_loss:1.8582548561010794\n",
      "Epoch 5 | train_loss:1.9223933295540754, valid_loss:2.0288148364836767\n",
      "Epoch 6 | train_loss:1.7832825663460723, valid_loss:1.5883344583295784\n",
      "Epoch 7 | train_loss:1.7702614473837073, valid_loss:1.508107812214264\n",
      "Epoch 8 | train_loss:1.8293371294116756, valid_loss:1.967834604642663\n",
      "Epoch 9 | train_loss:1.8121741930010018, valid_loss:1.8082911088966638\n",
      "Epoch 10 | train_loss:1.6970285474308497, valid_loss:2.024646382539038\n",
      "Epoch 11 | train_loss:1.6917722058967741, valid_loss:1.460263867806147\n",
      "Epoch 12 | train_loss:1.6921728334531743, valid_loss:1.5482684176340606\n",
      "Epoch 13 | train_loss:1.668588287009484, valid_loss:1.8389685208180264\n",
      "Epoch 14 | train_loss:1.669173691781292, valid_loss:1.516418561732652\n",
      "Epoch 15 | train_loss:1.6375561489485495, valid_loss:1.4526598767365493\n",
      "Epoch 16 | train_loss:1.6594027412861818, valid_loss:1.7093796557631018\n",
      "Epoch 17 | train_loss:1.702988743121076, valid_loss:1.520691227794355\n",
      "Epoch 18 | train_loss:1.655496701995963, valid_loss:1.4927773487966398\n",
      "Epoch 19 | train_loss:1.678270128572605, valid_loss:1.4222758283259809\n",
      "Epoch 20 | train_loss:1.693029463264914, valid_loss:1.7551411961349248\n",
      "Epoch 21 | train_loss:1.631840374899005, valid_loss:1.443972898366472\n",
      "Epoch 22 | train_loss:1.6301226076515285, valid_loss:1.4492830037959938\n",
      "Epoch 23 | train_loss:1.6095157691969557, valid_loss:2.0094056152612647\n",
      "Epoch 24 | train_loss:1.61772145509576, valid_loss:1.4236877571393858\n",
      "Epoch 25 | train_loss:1.5931420671379415, valid_loss:1.564610741516994\n",
      "Epoch 26 | train_loss:1.594503980224613, valid_loss:1.450690820422202\n",
      "Epoch 27 | train_loss:1.6258706067176578, valid_loss:1.3860864482278847\n",
      "Epoch 28 | train_loss:1.5861855635164377, valid_loss:1.4593467589858253\n",
      "Epoch 29 | train_loss:1.5684671046261653, valid_loss:1.5495964516192533\n",
      "Epoch 30 | train_loss:1.640989464139765, valid_loss:1.662121235164598\n",
      "Epoch 31 | train_loss:1.6486124495592183, valid_loss:1.486442728746362\n",
      "Epoch 32 | train_loss:1.627997883624689, valid_loss:1.4951694590244493\n",
      "Epoch 33 | train_loss:1.6176287657027124, valid_loss:1.4272413324151005\n",
      "Epoch 34 | train_loss:1.6058408132841695, valid_loss:1.4638079898441811\n",
      "Epoch 35 | train_loss:1.5970130918537957, valid_loss:1.4475526389882274\n",
      "Epoch 36 | train_loss:1.594700718740592, valid_loss:1.4884770860020724\n",
      "Epoch 37 | train_loss:1.5788706585112342, valid_loss:1.6248644969510644\n",
      "Epoch 38 | train_loss:1.584051635713902, valid_loss:1.4852360426397233\n",
      "Epoch 39 | train_loss:1.5664736142035893, valid_loss:1.587527071987455\n",
      "Epoch 40 | train_loss:1.5868045636642698, valid_loss:1.4307216443901363\n",
      "Epoch 41 | train_loss:1.559684770376312, valid_loss:1.4835332414183775\n",
      "Epoch 42 | train_loss:1.5778503602640859, valid_loss:1.474153971680601\n",
      "Epoch 43 | train_loss:1.5763180028388595, valid_loss:1.4101802873303806\n",
      "Epoch 44 | train_loss:1.6042831659872567, valid_loss:1.543779339951491\n",
      "Epoch 45 | train_loss:1.553122487927107, valid_loss:1.4149112621143145\n",
      "Epoch 46 | train_loss:1.5439474278591416, valid_loss:1.4765209067732001\n",
      "Epoch 47 | train_loss:1.5947545601342228, valid_loss:1.5322590592927112\n",
      "Epoch 48 | train_loss:1.547845120310252, valid_loss:1.3825086916697515\n",
      "Epoch 49 | train_loss:1.5606855682599505, valid_loss:1.4437284438944629\n",
      "Epoch 50 | train_loss:1.5774262134110597, valid_loss:1.6906325034148935\n"
     ]
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "epoch_num = 50\n",
    "target_idx = 1 # 0はじまり 0→分極率\n",
    "#targets = [\"Isotropic polarizability\", \"HOMO\", \"LUMO\", \"E_Gap\", \"Electronic spatial extent\", \"ZPVE\", \"U_0\", \"U\", \"H\", \"G\", \"Cv\", \"U_0 ATOM\", \"U ATOM\", \"H ATOM\", \"G ATOM\", \"A\", \"B\", \"C\"]\n",
    "#target_name = targets[target_idx]\n",
    "start = time.time() #時間計測開始\n",
    "results = []\n",
    "\n",
    "mse = F.mse_loss\n",
    "mae = F.l1_loss #mae\n",
    "\n",
    "def train(criterion):\n",
    "    # 学習前に毎回実行する\n",
    "    model = GCN_N(layer=layer,dim=dim)\n",
    "    # Optimizerの初期化\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(epoch_num):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        if criterion == mse:\n",
    "            train_loss = sqrt(train_loss)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        if criterion == mse:\n",
    "            valid_loss = sqrt(valid_loss)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "        results.append({\"Epoch\":epoch+1, \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "    return results\n",
    "\n",
    "#for layer in range(2,6):\n",
    "#    for i in range(5,8):\n",
    "#        dim = 2**i\n",
    "for layer in range(4,5):\n",
    "    for dim in [128]:    \n",
    "        print(\"RMSE\")\n",
    "        start = time.time()\n",
    "        results_mse = train(criterion=mse)\n",
    "        end = time.time()\n",
    "        time_diff = end - start\n",
    "        results_mse = (results_mse, time_diff)\n",
    "        #print(\"\")\n",
    "        #print(\"MAE\")\n",
    "        #results_mae = train(criterion=mae)\n",
    "\n",
    "        #results = [{\"Epoch\":i + 1, \"train_loss_RMSE\":results_mse[i][\"train_loss\"], \"valid_loss_RMSE\":results_mse[i][\"valid_loss\"], \"accuracy\":accuracy} for i in range(epoch_num)]\n",
    "        target_name = \"dipole\"\n",
    "        with open(f\"./results/GCN_{target_name}_{layer}_{dim}_{epoch_num}\", \"wb\") as f: #ファイル名：ターゲット、層数、隠れ層数、エポック数 (can be loaded by pickle)\n",
    "            #pickle.dump(results, f)\n",
    "            pickle.dump(results_mse, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'Epoch': 1,\n",
       "   'train_loss': 3.799780875091046,\n",
       "   'valid_loss': 2.1569826146665734},\n",
       "  {'Epoch': 2,\n",
       "   'train_loss': 2.4339145714010018,\n",
       "   'valid_loss': 2.208805357861647},\n",
       "  {'Epoch': 3,\n",
       "   'train_loss': 2.2565753938634185,\n",
       "   'valid_loss': 2.0834259701087148},\n",
       "  {'Epoch': 4,\n",
       "   'train_loss': 2.0575890960778933,\n",
       "   'valid_loss': 1.8582548561010794},\n",
       "  {'Epoch': 5,\n",
       "   'train_loss': 1.9223933295540754,\n",
       "   'valid_loss': 2.0288148364836767},\n",
       "  {'Epoch': 6,\n",
       "   'train_loss': 1.7832825663460723,\n",
       "   'valid_loss': 1.5883344583295784},\n",
       "  {'Epoch': 7,\n",
       "   'train_loss': 1.7702614473837073,\n",
       "   'valid_loss': 1.508107812214264},\n",
       "  {'Epoch': 8,\n",
       "   'train_loss': 1.8293371294116756,\n",
       "   'valid_loss': 1.967834604642663},\n",
       "  {'Epoch': 9,\n",
       "   'train_loss': 1.8121741930010018,\n",
       "   'valid_loss': 1.8082911088966638},\n",
       "  {'Epoch': 10,\n",
       "   'train_loss': 1.6970285474308497,\n",
       "   'valid_loss': 2.024646382539038},\n",
       "  {'Epoch': 11,\n",
       "   'train_loss': 1.6917722058967741,\n",
       "   'valid_loss': 1.460263867806147},\n",
       "  {'Epoch': 12,\n",
       "   'train_loss': 1.6921728334531743,\n",
       "   'valid_loss': 1.5482684176340606},\n",
       "  {'Epoch': 13,\n",
       "   'train_loss': 1.668588287009484,\n",
       "   'valid_loss': 1.8389685208180264},\n",
       "  {'Epoch': 14,\n",
       "   'train_loss': 1.669173691781292,\n",
       "   'valid_loss': 1.516418561732652},\n",
       "  {'Epoch': 15,\n",
       "   'train_loss': 1.6375561489485495,\n",
       "   'valid_loss': 1.4526598767365493},\n",
       "  {'Epoch': 16,\n",
       "   'train_loss': 1.6594027412861818,\n",
       "   'valid_loss': 1.7093796557631018},\n",
       "  {'Epoch': 17,\n",
       "   'train_loss': 1.702988743121076,\n",
       "   'valid_loss': 1.520691227794355},\n",
       "  {'Epoch': 18,\n",
       "   'train_loss': 1.655496701995963,\n",
       "   'valid_loss': 1.4927773487966398},\n",
       "  {'Epoch': 19,\n",
       "   'train_loss': 1.678270128572605,\n",
       "   'valid_loss': 1.4222758283259809},\n",
       "  {'Epoch': 20,\n",
       "   'train_loss': 1.693029463264914,\n",
       "   'valid_loss': 1.7551411961349248},\n",
       "  {'Epoch': 21,\n",
       "   'train_loss': 1.631840374899005,\n",
       "   'valid_loss': 1.443972898366472},\n",
       "  {'Epoch': 22,\n",
       "   'train_loss': 1.6301226076515285,\n",
       "   'valid_loss': 1.4492830037959938},\n",
       "  {'Epoch': 23,\n",
       "   'train_loss': 1.6095157691969557,\n",
       "   'valid_loss': 2.0094056152612647},\n",
       "  {'Epoch': 24,\n",
       "   'train_loss': 1.61772145509576,\n",
       "   'valid_loss': 1.4236877571393858},\n",
       "  {'Epoch': 25,\n",
       "   'train_loss': 1.5931420671379415,\n",
       "   'valid_loss': 1.564610741516994},\n",
       "  {'Epoch': 26,\n",
       "   'train_loss': 1.594503980224613,\n",
       "   'valid_loss': 1.450690820422202},\n",
       "  {'Epoch': 27,\n",
       "   'train_loss': 1.6258706067176578,\n",
       "   'valid_loss': 1.3860864482278847},\n",
       "  {'Epoch': 28,\n",
       "   'train_loss': 1.5861855635164377,\n",
       "   'valid_loss': 1.4593467589858253},\n",
       "  {'Epoch': 29,\n",
       "   'train_loss': 1.5684671046261653,\n",
       "   'valid_loss': 1.5495964516192533},\n",
       "  {'Epoch': 30,\n",
       "   'train_loss': 1.640989464139765,\n",
       "   'valid_loss': 1.662121235164598},\n",
       "  {'Epoch': 31,\n",
       "   'train_loss': 1.6486124495592183,\n",
       "   'valid_loss': 1.486442728746362},\n",
       "  {'Epoch': 32,\n",
       "   'train_loss': 1.627997883624689,\n",
       "   'valid_loss': 1.4951694590244493},\n",
       "  {'Epoch': 33,\n",
       "   'train_loss': 1.6176287657027124,\n",
       "   'valid_loss': 1.4272413324151005},\n",
       "  {'Epoch': 34,\n",
       "   'train_loss': 1.6058408132841695,\n",
       "   'valid_loss': 1.4638079898441811},\n",
       "  {'Epoch': 35,\n",
       "   'train_loss': 1.5970130918537957,\n",
       "   'valid_loss': 1.4475526389882274},\n",
       "  {'Epoch': 36,\n",
       "   'train_loss': 1.594700718740592,\n",
       "   'valid_loss': 1.4884770860020724},\n",
       "  {'Epoch': 37,\n",
       "   'train_loss': 1.5788706585112342,\n",
       "   'valid_loss': 1.6248644969510644},\n",
       "  {'Epoch': 38,\n",
       "   'train_loss': 1.584051635713902,\n",
       "   'valid_loss': 1.4852360426397233},\n",
       "  {'Epoch': 39,\n",
       "   'train_loss': 1.5664736142035893,\n",
       "   'valid_loss': 1.587527071987455},\n",
       "  {'Epoch': 40,\n",
       "   'train_loss': 1.5868045636642698,\n",
       "   'valid_loss': 1.4307216443901363},\n",
       "  {'Epoch': 41,\n",
       "   'train_loss': 1.559684770376312,\n",
       "   'valid_loss': 1.4835332414183775},\n",
       "  {'Epoch': 42,\n",
       "   'train_loss': 1.5778503602640859,\n",
       "   'valid_loss': 1.474153971680601},\n",
       "  {'Epoch': 43,\n",
       "   'train_loss': 1.5763180028388595,\n",
       "   'valid_loss': 1.4101802873303806},\n",
       "  {'Epoch': 44,\n",
       "   'train_loss': 1.6042831659872567,\n",
       "   'valid_loss': 1.543779339951491},\n",
       "  {'Epoch': 45,\n",
       "   'train_loss': 1.553122487927107,\n",
       "   'valid_loss': 1.4149112621143145},\n",
       "  {'Epoch': 46,\n",
       "   'train_loss': 1.5439474278591416,\n",
       "   'valid_loss': 1.4765209067732001},\n",
       "  {'Epoch': 47,\n",
       "   'train_loss': 1.5947545601342228,\n",
       "   'valid_loss': 1.5322590592927112},\n",
       "  {'Epoch': 48,\n",
       "   'train_loss': 1.547845120310252,\n",
       "   'valid_loss': 1.3825086916697515},\n",
       "  {'Epoch': 49,\n",
       "   'train_loss': 1.5606855682599505,\n",
       "   'valid_loss': 1.4437284438944629},\n",
       "  {'Epoch': 50,\n",
       "   'train_loss': 1.5774262134110597,\n",
       "   'valid_loss': 1.6906325034148935}],\n",
       " 1550.5883440971375)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/GCN_dipole_4_128_50\",\"rb\") as f:\n",
    "    a=pickle.load(f)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Epoch': 1,\n",
       "  'train_loss_RMSE': 1.309704071787473,\n",
       "  'valid_loss_RMSE': 1.2296099406769951},\n",
       " {'Epoch': 2,\n",
       "  'train_loss_RMSE': 1.2217893600422978,\n",
       "  'valid_loss_RMSE': 1.2276357297157248},\n",
       " {'Epoch': 3,\n",
       "  'train_loss_RMSE': 1.2077075167171785,\n",
       "  'valid_loss_RMSE': 1.1991930329111748},\n",
       " {'Epoch': 4,\n",
       "  'train_loss_RMSE': 1.199772980709421,\n",
       "  'valid_loss_RMSE': 1.21047771220476},\n",
       " {'Epoch': 5,\n",
       "  'train_loss_RMSE': 1.1888698123898245,\n",
       "  'valid_loss_RMSE': 1.1837441034624412},\n",
       " {'Epoch': 6,\n",
       "  'train_loss_RMSE': 1.1867064934371074,\n",
       "  'valid_loss_RMSE': 1.1928916204546156},\n",
       " {'Epoch': 7,\n",
       "  'train_loss_RMSE': 1.1786985627374724,\n",
       "  'valid_loss_RMSE': 1.1768997137012518},\n",
       " {'Epoch': 8,\n",
       "  'train_loss_RMSE': 1.1807449813929505,\n",
       "  'valid_loss_RMSE': 1.2215823209872976},\n",
       " {'Epoch': 9,\n",
       "  'train_loss_RMSE': 1.1775181536813815,\n",
       "  'valid_loss_RMSE': 1.2774895765572465},\n",
       " {'Epoch': 10,\n",
       "  'train_loss_RMSE': 1.1796164567712693,\n",
       "  'valid_loss_RMSE': 1.19750570731699},\n",
       " {'Epoch': 11,\n",
       "  'train_loss_RMSE': 1.1781516036209043,\n",
       "  'valid_loss_RMSE': 1.1635947128622492},\n",
       " {'Epoch': 12,\n",
       "  'train_loss_RMSE': 1.1838182606786576,\n",
       "  'valid_loss_RMSE': 1.2002221744873234},\n",
       " {'Epoch': 13,\n",
       "  'train_loss_RMSE': 1.1794938307609173,\n",
       "  'valid_loss_RMSE': 1.188980350475712},\n",
       " {'Epoch': 14,\n",
       "  'train_loss_RMSE': 1.1786596625245591,\n",
       "  'valid_loss_RMSE': 1.1767660881839517},\n",
       " {'Epoch': 15,\n",
       "  'train_loss_RMSE': 1.1779311343138505,\n",
       "  'valid_loss_RMSE': 1.1834826034886239},\n",
       " {'Epoch': 16,\n",
       "  'train_loss_RMSE': 1.170265686810039,\n",
       "  'valid_loss_RMSE': 1.2063814355592533},\n",
       " {'Epoch': 17,\n",
       "  'train_loss_RMSE': 1.187110982422167,\n",
       "  'valid_loss_RMSE': 1.1739813427372587},\n",
       " {'Epoch': 18,\n",
       "  'train_loss_RMSE': 1.1634019746827164,\n",
       "  'valid_loss_RMSE': 1.1943721860419187},\n",
       " {'Epoch': 19,\n",
       "  'train_loss_RMSE': 1.188801923074012,\n",
       "  'valid_loss_RMSE': 1.2524763815414135},\n",
       " {'Epoch': 20,\n",
       "  'train_loss_RMSE': 1.1915029187595516,\n",
       "  'valid_loss_RMSE': 1.194296338643688},\n",
       " {'Epoch': 21,\n",
       "  'train_loss_RMSE': 1.18875430912302,\n",
       "  'valid_loss_RMSE': 1.1789321747889627},\n",
       " {'Epoch': 22,\n",
       "  'train_loss_RMSE': 1.1849636363654517,\n",
       "  'valid_loss_RMSE': 1.1894244300918548},\n",
       " {'Epoch': 23,\n",
       "  'train_loss_RMSE': 1.182357705335472,\n",
       "  'valid_loss_RMSE': 1.191900800063934},\n",
       " {'Epoch': 24,\n",
       "  'train_loss_RMSE': 1.1908469112838647,\n",
       "  'valid_loss_RMSE': 1.1867747809091014},\n",
       " {'Epoch': 25,\n",
       "  'train_loss_RMSE': 1.1890654476273386,\n",
       "  'valid_loss_RMSE': 1.1860462482603777},\n",
       " {'Epoch': 26,\n",
       "  'train_loss_RMSE': 1.188578161110272,\n",
       "  'valid_loss_RMSE': 1.2143038180975088},\n",
       " {'Epoch': 27,\n",
       "  'train_loss_RMSE': 1.1858219200066884,\n",
       "  'valid_loss_RMSE': 1.2045202766296392},\n",
       " {'Epoch': 28,\n",
       "  'train_loss_RMSE': 1.1840114502244286,\n",
       "  'valid_loss_RMSE': 1.1887163269233874},\n",
       " {'Epoch': 29,\n",
       "  'train_loss_RMSE': 1.183783606869957,\n",
       "  'valid_loss_RMSE': 1.2025630531172713},\n",
       " {'Epoch': 30,\n",
       "  'train_loss_RMSE': 1.183276439834305,\n",
       "  'valid_loss_RMSE': 1.2068704410795459},\n",
       " {'Epoch': 31,\n",
       "  'train_loss_RMSE': 1.1855642360810996,\n",
       "  'valid_loss_RMSE': 1.2063744566580434},\n",
       " {'Epoch': 32,\n",
       "  'train_loss_RMSE': 1.1825532309644162,\n",
       "  'valid_loss_RMSE': 1.2009463054594045},\n",
       " {'Epoch': 33,\n",
       "  'train_loss_RMSE': 1.1811574129888993,\n",
       "  'valid_loss_RMSE': 1.1793392178833644},\n",
       " {'Epoch': 34,\n",
       "  'train_loss_RMSE': 1.1829162323122826,\n",
       "  'valid_loss_RMSE': 1.1813975374667482},\n",
       " {'Epoch': 35,\n",
       "  'train_loss_RMSE': 1.1807671981466537,\n",
       "  'valid_loss_RMSE': 1.1773202341338769},\n",
       " {'Epoch': 36,\n",
       "  'train_loss_RMSE': 1.179924785460701,\n",
       "  'valid_loss_RMSE': 1.2124537299008404},\n",
       " {'Epoch': 37,\n",
       "  'train_loss_RMSE': 1.1808914755260809,\n",
       "  'valid_loss_RMSE': 1.2038977382941252},\n",
       " {'Epoch': 38,\n",
       "  'train_loss_RMSE': 1.179793759171721,\n",
       "  'valid_loss_RMSE': 1.1756149044303454},\n",
       " {'Epoch': 39,\n",
       "  'train_loss_RMSE': 1.1808438054852186,\n",
       "  'valid_loss_RMSE': 1.2474226483094606},\n",
       " {'Epoch': 40,\n",
       "  'train_loss_RMSE': 1.18162915634656,\n",
       "  'valid_loss_RMSE': 1.1899680356147166},\n",
       " {'Epoch': 41,\n",
       "  'train_loss_RMSE': 1.1810264452564136,\n",
       "  'valid_loss_RMSE': 1.1795309078625107},\n",
       " {'Epoch': 42,\n",
       "  'train_loss_RMSE': 1.179268568532052,\n",
       "  'valid_loss_RMSE': 1.2083303224968418},\n",
       " {'Epoch': 43,\n",
       "  'train_loss_RMSE': 1.1807135383764367,\n",
       "  'valid_loss_RMSE': 1.1950929279636646},\n",
       " {'Epoch': 44,\n",
       "  'train_loss_RMSE': 1.181254562548839,\n",
       "  'valid_loss_RMSE': 1.1931706140485827},\n",
       " {'Epoch': 45,\n",
       "  'train_loss_RMSE': 1.1807967804597423,\n",
       "  'valid_loss_RMSE': 1.1909463864870402},\n",
       " {'Epoch': 46,\n",
       "  'train_loss_RMSE': 1.1827085532612516,\n",
       "  'valid_loss_RMSE': 1.1850368942981098},\n",
       " {'Epoch': 47,\n",
       "  'train_loss_RMSE': 1.1721249565615077,\n",
       "  'valid_loss_RMSE': 1.1932434246371035},\n",
       " {'Epoch': 48,\n",
       "  'train_loss_RMSE': 1.1644612206072145,\n",
       "  'valid_loss_RMSE': 1.172758142407005},\n",
       " {'Epoch': 49,\n",
       "  'train_loss_RMSE': 1.1591606348244454,\n",
       "  'valid_loss_RMSE': 1.1629745708462467},\n",
       " {'Epoch': 50,\n",
       "  'train_loss_RMSE': 1.1598628110478408,\n",
       "  'valid_loss_RMSE': 1.1660579505564965},\n",
       " {'Epoch': 1,\n",
       "  'train_loss': 1.2931240012301832,\n",
       "  'valid_loss': 1.2216961634897083},\n",
       " {'Epoch': 2,\n",
       "  'train_loss': 1.2330032547603955,\n",
       "  'valid_loss': 1.229042390855985},\n",
       " {'Epoch': 3,\n",
       "  'train_loss': 1.215020012371159,\n",
       "  'valid_loss': 1.201417569395244},\n",
       " {'Epoch': 4,\n",
       "  'train_loss': 1.2058880915221108,\n",
       "  'valid_loss': 1.194955782971834},\n",
       " {'Epoch': 5,\n",
       "  'train_loss': 1.2034668122165486,\n",
       "  'valid_loss': 1.1884973107135435},\n",
       " {'Epoch': 6,\n",
       "  'train_loss': 1.1987118850910377,\n",
       "  'valid_loss': 1.1977032763822386},\n",
       " {'Epoch': 7,\n",
       "  'train_loss': 1.197952170520657,\n",
       "  'valid_loss': 1.199957386879708},\n",
       " {'Epoch': 8,\n",
       "  'train_loss': 1.1928606379876323,\n",
       "  'valid_loss': 1.1889358646058807},\n",
       " {'Epoch': 9,\n",
       "  'train_loss': 1.1948765039600644,\n",
       "  'valid_loss': 1.1911381079970165},\n",
       " {'Epoch': 10,\n",
       "  'train_loss': 1.1945802077689032,\n",
       "  'valid_loss': 1.2090092399871082},\n",
       " {'Epoch': 11,\n",
       "  'train_loss': 1.195046856459039,\n",
       "  'valid_loss': 1.194508070685323},\n",
       " {'Epoch': 12,\n",
       "  'train_loss': 1.1896331800334665,\n",
       "  'valid_loss': 1.1968604250448838},\n",
       " {'Epoch': 13,\n",
       "  'train_loss': 1.1905384585077525,\n",
       "  'valid_loss': 1.2142827212834861},\n",
       " {'Epoch': 14,\n",
       "  'train_loss': 1.1904282704043248,\n",
       "  'valid_loss': 1.183730193647631},\n",
       " {'Epoch': 15,\n",
       "  'train_loss': 1.1898159640038428,\n",
       "  'valid_loss': 1.2140025642876138},\n",
       " {'Epoch': 16,\n",
       "  'train_loss': 1.1875294059200625,\n",
       "  'valid_loss': 1.1793723057380914},\n",
       " {'Epoch': 17,\n",
       "  'train_loss': 1.1898627909382442,\n",
       "  'valid_loss': 1.178953721887677},\n",
       " {'Epoch': 18,\n",
       "  'train_loss': 1.188897320859842,\n",
       "  'valid_loss': 1.1960986356248346},\n",
       " {'Epoch': 19,\n",
       "  'train_loss': 1.186872605463106,\n",
       "  'valid_loss': 1.1773516701328568},\n",
       " {'Epoch': 20,\n",
       "  'train_loss': 1.1868485549721504,\n",
       "  'valid_loss': 1.1884238626813397},\n",
       " {'Epoch': 21,\n",
       "  'train_loss': 1.1873853355477928,\n",
       "  'valid_loss': 1.1903271835085427},\n",
       " {'Epoch': 22,\n",
       "  'train_loss': 1.1879238757328936,\n",
       "  'valid_loss': 1.1848066384545939},\n",
       " {'Epoch': 23,\n",
       "  'train_loss': 1.1885369734163536,\n",
       "  'valid_loss': 1.2014950865970364},\n",
       " {'Epoch': 24,\n",
       "  'train_loss': 1.1875359284830405,\n",
       "  'valid_loss': 1.2024491213495856},\n",
       " {'Epoch': 25,\n",
       "  'train_loss': 1.1879590647173301,\n",
       "  'valid_loss': 1.1820709044499056},\n",
       " {'Epoch': 26,\n",
       "  'train_loss': 1.1886360415781316,\n",
       "  'valid_loss': 1.190976392932982},\n",
       " {'Epoch': 27,\n",
       "  'train_loss': 1.1856141798327282,\n",
       "  'valid_loss': 1.1847938042255448},\n",
       " {'Epoch': 28,\n",
       "  'train_loss': 1.1882803136777638,\n",
       "  'valid_loss': 1.1810474359284493},\n",
       " {'Epoch': 29,\n",
       "  'train_loss': 1.1854303446260652,\n",
       "  'valid_loss': 1.21650552771776},\n",
       " {'Epoch': 30,\n",
       "  'train_loss': 1.1874154643908188,\n",
       "  'valid_loss': 1.2160605516220577},\n",
       " {'Epoch': 31,\n",
       "  'train_loss': 1.1858351527238489,\n",
       "  'valid_loss': 1.1932869153962675},\n",
       " {'Epoch': 32,\n",
       "  'train_loss': 1.1902876256409851,\n",
       "  'valid_loss': 1.2242285531190051},\n",
       " {'Epoch': 33,\n",
       "  'train_loss': 1.1860437040774248,\n",
       "  'valid_loss': 1.1892035009563153},\n",
       " {'Epoch': 34,\n",
       "  'train_loss': 1.1865596627093655,\n",
       "  'valid_loss': 1.2069967100165424},\n",
       " {'Epoch': 35,\n",
       "  'train_loss': 1.1848143882455318,\n",
       "  'valid_loss': 1.1996569422411192},\n",
       " {'Epoch': 36,\n",
       "  'train_loss': 1.1876938748559172,\n",
       "  'valid_loss': 1.1924486359336333},\n",
       " {'Epoch': 37,\n",
       "  'train_loss': 1.187238248479395,\n",
       "  'valid_loss': 1.1750818625722868},\n",
       " {'Epoch': 38,\n",
       "  'train_loss': 1.1860424836911285,\n",
       "  'valid_loss': 1.1881894847436196},\n",
       " {'Epoch': 39,\n",
       "  'train_loss': 1.187448651386805,\n",
       "  'valid_loss': 1.199186617376403},\n",
       " {'Epoch': 40,\n",
       "  'train_loss': 1.187502569366066,\n",
       "  'valid_loss': 1.1911175581701325},\n",
       " {'Epoch': 41,\n",
       "  'train_loss': 1.1863130988405537,\n",
       "  'valid_loss': 1.2305788518611187},\n",
       " {'Epoch': 42,\n",
       "  'train_loss': 1.1876415857252802,\n",
       "  'valid_loss': 1.198811620218347},\n",
       " {'Epoch': 43,\n",
       "  'train_loss': 1.1852835255750285,\n",
       "  'valid_loss': 1.2331278924470102},\n",
       " {'Epoch': 44,\n",
       "  'train_loss': 1.188429560282174,\n",
       "  'valid_loss': 1.1741371562274434},\n",
       " {'Epoch': 45,\n",
       "  'train_loss': 1.1851445456350846,\n",
       "  'valid_loss': 1.1790595263020938},\n",
       " {'Epoch': 46,\n",
       "  'train_loss': 1.1849512499962893,\n",
       "  'valid_loss': 1.2419789716993668},\n",
       " {'Epoch': 47,\n",
       "  'train_loss': 1.1868192817628387,\n",
       "  'valid_loss': 1.180749652057654},\n",
       " {'Epoch': 48,\n",
       "  'train_loss': 1.186824831438302,\n",
       "  'valid_loss': 1.1726630201045012},\n",
       " {'Epoch': 49,\n",
       "  'train_loss': 1.1850198619775765,\n",
       "  'valid_loss': 1.1881586082105045},\n",
       " {'Epoch': 50,\n",
       "  'train_loss': 1.1876905557897262,\n",
       "  'valid_loss': 1.207958249801897}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Epoch': 1,\n",
       "  'train_loss_RMSE': 1.3171492476344866,\n",
       "  'valid_loss_RMSE': 1.2604073345344995,\n",
       "  'train_loss_MAE': 1.3171492476344866,\n",
       "  'valid_loss_MAE': 1.2604073345344995},\n",
       " {'Epoch': 2,\n",
       "  'train_loss_RMSE': 1.2160129461851614,\n",
       "  'valid_loss_RMSE': 1.274314754044402,\n",
       "  'train_loss_MAE': 1.2160129461851614,\n",
       "  'valid_loss_MAE': 1.274314754044402}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/GCN_dipole_2_32_2\", \"rb\") as f:\n",
    "    test1 = pickle.load(f)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newarray = np.ndarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "\n",
    "target_idx = 1\n",
    "num_epochs = 10\n",
    "fold = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "for f, (train_idx,valid_idx) in enumerate(fold.split(np.arange(len(dataset)))):\n",
    "    \n",
    "    print(f\"Fold {f + 1}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    # train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_graphs = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "            optimizer.step()\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        train_loss = sqrt(train_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                #loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "                loss = criterion(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                total_graphs += batch.num_graphs\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_loss = sqrt(valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.5111434600112155,\n",
       " 3.4445386044421165,\n",
       " 3.4405643966218706,\n",
       " 3.437776689995181,\n",
       " 3.418383062939326]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"pickle_test\",\"rb\") as f:\n",
    "    pickle_test = pickle.load(f)\n",
    "pickle_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ層の数は層によって変えるべき？\n",
    "\n",
    "edge_attr:多次元\n",
    "\n",
    "edge_weight:一次元\n",
    "\n",
    "GCNはedge_weightのため、QM9のedge_attrが使えない。そのため、edge_attrなしでの計算になる\n",
    "\n",
    "GATはedge_attrが使える。edge_attrの追加によって悪化した。\n",
    "GAT2はedge_attrが使えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 具体的な予[測値\n",
    "[[prediction[i].item(),batch.y[:, target_idx][i].item()] for i in range(len(prediction))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
