{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "\n",
    "dataset = QM9(root=\"./QM9\")\n",
    "#無向グラフの例\n",
    "#edge_index = torch.tensor([[0,1,1,2],[1,0,2,1]], dtype=torch.long) # エッジの定義\n",
    "#x = torch.tensor([[-1],[0],[1]], dtype=torch.float) # ノードの属性\n",
    "#data = Data(x=x, edge_index=edge_index) # コンストラクタ\n",
    "# Data(x=[3, 1], edge_index=[2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度予測\n",
    "\n",
    "def ECFPGen(smiles, radius=4, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return morgan\n",
    "    \n",
    "def npECFP(morgan):\n",
    "    array = np.zeros(morgan.GetNumBits())\n",
    "    rdkit.DataStructs.ConvertToNumpyArray(morgan, array)\n",
    "    return np.nonzero(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubchempy\n",
    "test1 = pubchempy.get_properties([\"CanonicalSMILES\"], \"18760-80-0\", \"name\")\n",
    "test2 = pubchempy.get_properties([\"CanonicalSMILES\"], \"amphetamine\", \"name\")\n",
    "smiles1 = test1[0][\"CanonicalSMILES\"]\n",
    "smiles2 = test2[0][\"CanonicalSMILES\"]\n",
    "rdkit.DataStructs.FingerprintSimilarity(ECFPGen(smiles1), ECFPGen(smiles2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# 回帰の手法(https://chemrxiv.org/engage/chemrxiv/article-details/60c75208bdbb899737a3a1c2)\n",
    "# MLP, kNN, KRR, SVM, RF, LightGBM, GBRT\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math\n",
    "random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "ECFP = [list(ECFPGen(smiles)) for smiles in df[\"smiles\"].values]\n",
    "\n",
    "# 説明変数と説明変数(α)\n",
    "X = [np.array(i) for i in ECFP]\n",
    "Y = df[\"alpha\"].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "#1m54.2でできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "#SVM = LinearSVR(loss=\"squared_epsilon_insensitive\", random_state=0, max_iter=10000)\n",
    "SVM = SVR(kernel=\"linear\", max_iter=10000)\n",
    "SVM.fit(X_train, Y_train)\n",
    "Y_pred = SVM.predict(X_test)\n",
    "mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9639885947707616"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=4, max_iter=1000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "mlp.fit(X_train,Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "error = mean_squared_error(Y_test, Y_pred, squared=False)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8930943694393636"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_mae = mean_absolute_error(Y_test, Y_pred)\n",
    "error_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "errors = []\n",
    "for i in range(1, 10):\n",
    "    train_X = X[:80]\n",
    "    train_Y = Y[:80]\n",
    "    test_X = X[80:]\n",
    "    test_Y = Y[80:]\n",
    "    hidden_layer_sizes = 2**i\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=100000, activation=\"relu\", solver=\"adam\", random_state=0)\n",
    "    mlp.fit(train_X,train_Y)\n",
    "    pred_Y = mlp.predict(test_X)\n",
    "    error = mean_squared_error(test_Y, pred_Y, squared=False)\n",
    "    errors.append((i,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in errors], [i[1] for i in errors]) \n",
    "plt.xlabel(\"number of hidden layers(2^x)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"max_iter=100000, adam, relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch ECFP\n",
    "X_tensor = torch.tensor(X)\n",
    "Y_tensor = torch.tensor(Y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset(For understanding)\n",
    "# reference source code \n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/qm9.html\n",
    "# neural fingerprint on pytorch ref\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "\n",
    "\n",
    "#RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "# units conversion\n",
    "HAR2EV = 27.211386246\n",
    "KCALMOL2EV = 0.04336414\n",
    "conversion = torch.tensor([\n",
    "1., 1., HAR2EV, HAR2EV, HAR2EV, 1., HAR2EV, HAR2EV, HAR2EV, HAR2EV, HAR2EV,\n",
    "1., KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, 1., 1., 1.\n",
    "])\n",
    "\n",
    "atomrefs = {\n",
    "    6: [0., 0., 0., 0., 0.],\n",
    "    7: [\n",
    "        -13.61312172, -1029.86312267, -1485.30251237, -2042.61123593,\n",
    "        -2713.48485589\n",
    "    ],\n",
    "    8: [\n",
    "        -13.5745904, -1029.82456413, -1485.26398105, -2042.5727046,\n",
    "        -2713.44632457\n",
    "    ],\n",
    "    9: [\n",
    "        -13.54887564, -1029.79887659, -1485.2382935, -2042.54701705,\n",
    "        -2713.42063702\n",
    "    ],\n",
    "    10: [\n",
    "        -13.90303183, -1030.25891228, -1485.71166277, -2043.01812778,\n",
    "        -2713.88796536\n",
    "    ],\n",
    "    11: [0., 0., 0., 0., 0.],\n",
    "}\n",
    "\n",
    "class MyFirstDataset(pyg.data.Dataset):\n",
    "    def __init__(self, root=\"./MyFirstDataset\", transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return \"qm9.pt\"\n",
    "\n",
    "    def process(self):\n",
    "        types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\n",
    "        bonds = {BondType.SINGLE: 0, BondType.DOUBLE: 1, BondType.TRIPLE: 2, BondType.AROMATIC: 3}\n",
    "        \n",
    "        # グラフの特徴量\n",
    "        self.data = pd.read_csv(\"qm9_dataset.csv\")\n",
    "            # 列の並べ替え(reindex)\n",
    "        self.data = self.data.reindex(index=[\"mol_id\", \"smiles\", \"alpha\", \"homo\", \"lumo\", \"gap\", \"r2\", \"zpve\", \"u0\", \"u298\", \"h298\", \"g298\", \"cv\", \"u0_atom\", \"u298_atom\", \"h298_atom\", \"g298_atom\", \"A\", \"B\", \"C\"])\n",
    "        \n",
    "        for name, i in df.iterrows():\n",
    "            target = [float(x) for x in i.values[2:]]\n",
    "            target = torch.tensor(target, dtype=torch.float)\n",
    "            target = torch.cat([target[:, 3:], target[:, 3:]], dim=0) # ?\n",
    "            target = target * conversion.view(1, -1)\n",
    "\n",
    "        for index, mol in self.data.iterrows():\n",
    "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
    "            N = mol_obj.GetNumAtoms()\n",
    "            conf = mol.GetConformer()\n",
    "            pos = conf.GetPositions()\n",
    "            pos = torch.tensor(pos, dtype=torch.float)\n",
    "\n",
    "            type_idx = atomic_number = aromatic = sp = sp2 = sp3 = num_hs = [], [], [], [], [], [], []\n",
    "            for atom in mol_obj.GetAtoms():\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "                atomic_number.append(atom.GetAtomicNum())\n",
    "                if atom.GetIsAromatic():\n",
    "                    aromatic.append(1)\n",
    "                else:\n",
    "                    aromatic.append(0)\n",
    "                hybridization = atom.GetHybridization()\n",
    "                sp.append(1 if hybridization == HybridizationType.SP else 0)\n",
    "                sp2.append(1 if hybridization == HybridizationType.SP2 else 0)\n",
    "                sp3.append(1 if hybridization == HybridizationType.SP3 else 0)\n",
    "            z = torch.tensor(atomic_number, dtype=torch.long)\n",
    "            \n",
    "            # edge index\n",
    "            row, col, edge_type = [], [], []\n",
    "            for bond in mol_obj.GetBonds():\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                edge_type += 2 * [bonds[bond.GetBondType()]]\n",
    "            \n",
    "            edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "            edge_attr = one_hot(edge_type, num_classes=len(bonds))\n",
    "\n",
    "            perm = (edge_index[0] * N + edge_index[1]).argsort()\n",
    "            edge_index = edge_index[:, perm]\n",
    "            edge_type = edge_type[perm]\n",
    "            edge_attr = edge_attr[perm]\n",
    "\n",
    "            row, col = edge_index\n",
    "            hs = (z == 1).to(torch.float)\n",
    "            num_hs = scatter(hs[row], col, dim_size=N, reduce='sum').tolist()\n",
    "            # types: qm9に含まれる原子の一覧(H,C,N,O,F)。num_classesは原子の種類の数\n",
    "            # x1:原子記号のリスト\n",
    "            x1 = one_hot(torch.tensor(type_idx), num_classes=len(types))\n",
    "            \n",
    "            # node features: 原子番号、芳香性、混成の有無(sp,sp2,sp3)、水素の数\n",
    "            x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs],\n",
    "                              dtype=torch.float).t().contiguous()\n",
    "            # x1,x2を結合\n",
    "            x = torch.cat([x1, x2], dim=-1)\n",
    "            #グラフ特徴量\n",
    "            y = target[i].unsqueeze(0)\n",
    "\n",
    "            # node features\n",
    "            node_features = self._get_node_features(mol_obj)\n",
    "            # edge features\n",
    "            edge_features = self._get_edge_features(mol_obj)\n",
    "            # edge inde\n",
    "            edge_index = self._get_adjacency_info(mol_obj)\n",
    "            # label\n",
    "            label = self.__get__labels(mol[\"alpha\"])\n",
    "            # create dataset\n",
    "            data = pyg.Data(x=node_features,\n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_features,\n",
    "                            y=label,\n",
    "                            smiles=mol[\"smiles\"])\n",
    "            torch.save(Data, os.path.join(self.processed_dir, f\"data_{index}.pt\"))\n",
    "    \n",
    "    def _get_node_features(self, mol):\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features = []\n",
    "            node_features.append(atom.GetAtomicNum())\n",
    "            node_features.append(atom.GetDegree())\n",
    "            node_features.append(GetFormalCharge())\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QM9の属性\n",
    "\n",
    "x:ノードの特徴量(原子数×特徴量数=11)\n",
    "\n",
    "y:ラベル(ラベル数)\n",
    "\n",
    "z:原子番号(原子数)\n",
    "\n",
    "edge_attr:エッジ特徴量=結合次数(エッジ数×結合次数)\n",
    "\n",
    "edge_index:エッジリスト(2×エッジ数)\n",
    "\n",
    "pos:3Dグリッドでの各原子の位置(原子数×3)\n",
    "\n",
    "正則化の手法\n",
    "・L1正則化(重み減衰)\n",
    "・L2正則化(重み減衰)\n",
    "・Dropout\n",
    "・ラベル平滑化\n",
    "・バッチ正則化\n",
    "・"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GATv2_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, self.dim)\n",
    "        self.convn = GATv2Conv(self.dim, self.dim)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GATv2Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, dataset.num_node_features*32))\n",
    "        self.conv2_net = torch.nn.Sequential(nn.Linear(dataset.num_edge_features, 32),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(32, 32*16))        \n",
    "        self.conv1 = NNConv(dataset.num_node_features, 32, self.conv1_net)\n",
    "        self.conv2 = NNConv(32, 16, self.conv2_net)\n",
    "        self.linear1 = torch.nn.Linear(16, 32)\n",
    "        self.out = nn.Linear(32,1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECFP\n",
    "# https://qiita.com/kimisyo/items/55a01e27aa03852d84e9\n",
    "# https://pubs.acs.org/doi/10.1021/acsomega.1c01266\n",
    "# https://pubs.acs.org/doi/10.1021/acs.jcim.0c01208\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./qm9_dataset.csv\")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "import numpy as np\n",
    "\n",
    "def ECFPGen(smiles, radius=3, nBits=12):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    bit_morgan1 = {}\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, bitInfo=bit_morgan1)\n",
    "    bit1 = list(fp1)\n",
    "    return bit1\n",
    "\n",
    "df[\"ECFP\"] =  [ECFPGen(smiles, radius=2, nBits=2048) for smiles in df[\"smiles\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset作成\n",
    "polar = df[\"alpha\"]\n",
    "ECFP = df[\"ECFP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.6), int(len(dataset)*0.2)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 32\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "layer = 2\n",
    "dim = 32\n",
    "model = GATv2_N(layer=layer, dim=dim) \n",
    "# 損失関数\n",
    "criterion = F.mse_loss\n",
    "# Optimizerの初期化\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(params=lr=0.01, weight_decay=5e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss:9.07853698983072, valid_loss:4.042524073701033\n",
      "Epoch 2 | train_loss:7.496221716460311, valid_loss:3.384956860309125\n",
      "Epoch 3 | train_loss:6.785162524566868, valid_loss:3.6569578318257787\n",
      "Epoch 4 | train_loss:6.199327667845685, valid_loss:2.9255349150788232\n",
      "Epoch 5 | train_loss:5.628869287930763, valid_loss:2.9149654911316403\n",
      "Epoch 6 | train_loss:5.373914212887134, valid_loss:5.328505331846205\n",
      "Epoch 7 | train_loss:5.165792439463863, valid_loss:3.3188325344496077\n",
      "Epoch 8 | train_loss:5.0134591041975325, valid_loss:3.411937604760774\n",
      "Epoch 9 | train_loss:4.875557406704505, valid_loss:4.461915556259435\n",
      "Epoch 10 | train_loss:4.820036825360374, valid_loss:4.589953398937701\n",
      "Epoch 11 | train_loss:4.756704683311606, valid_loss:3.4218967516148293\n",
      "Epoch 12 | train_loss:4.703498927771608, valid_loss:4.271243323032314\n",
      "Epoch 13 | train_loss:4.671608169484935, valid_loss:3.450758718628172\n",
      "Epoch 14 | train_loss:4.646545784300483, valid_loss:3.5533131197787147\n",
      "Epoch 15 | train_loss:4.63191296350888, valid_loss:3.4313511618192156\n",
      "Epoch 16 | train_loss:4.60575788552526, valid_loss:3.9907847730338135\n",
      "Epoch 17 | train_loss:4.612069715594702, valid_loss:3.462290141868125\n",
      "Epoch 18 | train_loss:4.5872136916974355, valid_loss:3.5301728255882825\n",
      "Epoch 19 | train_loss:4.581941531650789, valid_loss:3.6129203293026224\n",
      "Epoch 20 | train_loss:4.610491934693141, valid_loss:4.100384382863499\n",
      "Epoch 21 | train_loss:4.563407390605364, valid_loss:3.4758560405383774\n",
      "Epoch 22 | train_loss:4.569256958285288, valid_loss:3.4895435031002484\n",
      "Epoch 23 | train_loss:4.56277731510324, valid_loss:3.7328748023888885\n",
      "Epoch 24 | train_loss:4.558282413323331, valid_loss:4.682062302941507\n",
      "Epoch 25 | train_loss:4.538847537281655, valid_loss:3.6481426705066617\n",
      "Epoch 26 | train_loss:4.550709109531735, valid_loss:3.8583214539478927\n",
      "Epoch 27 | train_loss:4.5360967736955375, valid_loss:4.3871155041645675\n",
      "Epoch 28 | train_loss:4.530553863606223, valid_loss:4.3023131322161206\n",
      "Epoch 29 | train_loss:4.53538943978957, valid_loss:3.6099464811730795\n",
      "Epoch 30 | train_loss:4.5519257511855535, valid_loss:3.5837736523238837\n",
      "Epoch 31 | train_loss:4.533642897298886, valid_loss:3.563075875740471\n",
      "Epoch 32 | train_loss:4.541074418302659, valid_loss:3.472070242635778\n",
      "Epoch 33 | train_loss:4.545019523147266, valid_loss:3.632178041054742\n",
      "Epoch 34 | train_loss:4.49084998323554, valid_loss:3.6629935849849637\n",
      "Epoch 35 | train_loss:4.531311503540916, valid_loss:3.549317051991274\n",
      "Epoch 36 | train_loss:4.498870783738227, valid_loss:3.4469000638842875\n",
      "Epoch 37 | train_loss:4.48332390779388, valid_loss:3.793453037301602\n",
      "Epoch 38 | train_loss:4.493740877390102, valid_loss:3.5010477866986562\n",
      "Epoch 39 | train_loss:4.512375102940865, valid_loss:4.419419980865236\n",
      "Epoch 40 | train_loss:4.512149560908137, valid_loss:3.663286603167471\n",
      "Epoch 41 | train_loss:4.531558160434045, valid_loss:3.7621360937074226\n",
      "Epoch 42 | train_loss:4.4892619717091975, valid_loss:3.6473528149658425\n",
      "Epoch 43 | train_loss:4.483120396356808, valid_loss:3.570338764954313\n",
      "Epoch 44 | train_loss:4.490386485956312, valid_loss:3.4555700057001744\n",
      "Epoch 45 | train_loss:4.4939865759940485, valid_loss:3.6793198678779135\n",
      "Epoch 46 | train_loss:4.502113638122391, valid_loss:3.5400329351716637\n",
      "Epoch 47 | train_loss:4.499776923326136, valid_loss:3.3822697489943656\n",
      "Epoch 48 | train_loss:4.469559851757569, valid_loss:3.9184560950635987\n",
      "Epoch 49 | train_loss:4.474561364454473, valid_loss:4.002675771421791\n",
      "Epoch 50 | train_loss:4.453532411300174, valid_loss:3.3802765584516643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nloss_two_50.append(\\n    {\\n        \"model\": \"GCN\",\\n        \"layer\": layer,\\n        \"dim\": dim,\\n        \"batch_size\": 32,\\n        \"loss\": \"RMSE\",\\n        \"lr\": 0.01,\\n        \"decay\": 5e-4,\\n        \"seed\": seed,\\n        \"data_split\":[\\n            0.6,\\n            0.6,\\n            0.2\\n        ],\\n        \"time\": round(used_time, 4)\\n    }\\n)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "loss_two_50 = []\n",
    "start = time.time() #時間計測開始\n",
    "for epoch in range(50):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_graphs = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(batch)\n",
    "        loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    train_loss /=  len(train_loader) #損失の平均(batchあたり)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    total_graphs = 0\n",
    "    with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "        for batch in valid_loader:\n",
    "            prediction = model(batch)\n",
    "            loss = torch.sqrt(criterion(prediction, batch.y[:, target_idx].unsqueeze(1)))\n",
    "            valid_loss += loss.item()\n",
    "            total_graphs += batch.num_graphs\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "    #loss_three_50.append({\"Epoch\":epoch + 1 , \"train_loss\":train_loss, \"valid_loss\":valid_loss})\n",
    "used_time = time.time() - start\n",
    "\"\"\"\n",
    "loss_two_50.append(\n",
    "    {\n",
    "        \"model\": \"GCN\",\n",
    "        \"layer\": layer,\n",
    "        \"dim\": dim,\n",
    "        \"batch_size\": 32,\n",
    "        \"loss\": \"RMSE\",\n",
    "        \"lr\": 0.01,\n",
    "        \"decay\": 5e-4,\n",
    "        \"seed\": seed,\n",
    "        \"data_split\":[\n",
    "            0.6,\n",
    "            0.6,\n",
    "            0.2\n",
    "        ],\n",
    "        \"time\": round(used_time, 4)\n",
    "    }\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_two_50 = loss_three_50[51:]\n",
    "\n",
    "loss_two_50.append(\n",
    "    {\n",
    "        \"model\": \"GCN\",\n",
    "        \"layer\": layer,\n",
    "        \"dim\": dim,\n",
    "        \"batch_size\": 32,\n",
    "        \"loss\": \"RMSE\",\n",
    "        \"lr\": 0.01,\n",
    "        \"decay\": 5e-4,\n",
    "        \"seed\": seed,\n",
    "        \"data_split\":[\n",
    "            0.6,\n",
    "            0.6,\n",
    "            0.2\n",
    "        ],\n",
    "        \"time\": round(used_time, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./loss_two_50.json\", \"a\") as f:\n",
    "    json.dump(loss_two_50, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./loss_two_50.json\", \"r\") as f:\n",
    "    two = json.load(f)\n",
    "dim16 = two[1][:-1]\n",
    "dim32 = two[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層\n",
    "epoch = [i for i in range(1, 51)] \n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim16])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim16])\n",
    "plt.title(\"dim=16\")\n",
    "plt.ylim(0,14)\n",
    "plt.subplot(122)\n",
    "plt.plot(epoch, [i[\"train_loss\"] for i in dim32])\n",
    "plt.plot(epoch, [i[\"valid_loss\"] for i in dim32])\n",
    "plt.title(\"dim=32\")\n",
    "plt.ylim(0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# お前はもう必要ない\n",
    "import re\n",
    "def parser(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    loss = [{\"train_loss\":re.sub(\"train_loss:\", \"\", i.split(\"| \")[1].split(\",\")[0]), \"valid_loss\":re.sub(\" valid_loss:\", \"\", i.split(\"| \")[1].split(\",\")[1])}for i in text if i]\n",
    "    train_loss = [float(i[\"train_loss\"]) for i in loss]\n",
    "    valid_loss = [float(i[\"valid_loss\"]) for i in loss]\n",
    "    return np.array([train_loss, valid_loss])\n",
    "loss_two = parser(two_layers)\n",
    "loss_three = parser(theree_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "epoch = [i for i in range(1, len(loss_two[0]) + 1)]\n",
    "plt.subplot(1,2,1) \n",
    "loss_two = np.log(loss_two)\n",
    "plt.plot(epoch, loss_two[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_two[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3層\n",
    "import matplotlib.pyplot as plt\n",
    "epoch = [i for i in range(1, len(loss_three[0]) + 1)]\n",
    "loss_three = np.log(loss_three)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, loss_three[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, loss_three[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 評価開始\n",
    "predictions = []\n",
    "real = []\n",
    "for batch in test_loader:\n",
    "    output = model(batch.to(\"cpu\"))\n",
    "    predictions.append(output.detach().cpu().numpy())\n",
    "    real.append(batch.y[:,target_idx].detach().cpu().numpy())\n",
    "real = np.concatenate(real)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "plt.scatter(real, predictions)\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('real')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Your Own Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "\n",
    "https://qiita.com/maskot1977/items/4aa6322459eb3a78955f\n",
    "\n",
    "\n",
    "Datasets\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html\n",
    "\n",
    "\n",
    "TORCH.NN.FUNCTIONAL\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# エタンのグラフ構造の作成\n",
    "mol = Chem.MolFromSmiles(\"CC\")\n",
    "mol = Chem.AddHs(mol)\n",
    "atoms = mol.GetAtoms()\n",
    "bonds = mol.GetBonds()\n",
    "bonds[0].GetEndAtomIdx()\n",
    "\n",
    "edge_list = []\n",
    "for bond in bonds:\n",
    "    edge_list.append([bond.GetBeginAtomIdx(),bond.GetEndAtomIdx()])\n",
    "    edge_list.append([bond.GetEndAtomIdx(),bond.GetBeginAtomIdx()])\n",
    "edge_index = torch.tensor(edge_list) #エッジのリスト作成\n",
    "x = torch.tensor([[atom.GetAtomicNum()] for atom in atoms]) # 原子番号\n",
    "\n",
    "edge_attr = []\n",
    "for bond in bonds:\n",
    "    edge_attr.append([])\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ構造の可視化\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_networkx\n",
    "from IPython.display import SVG, display\n",
    "data = dataset[4921]\n",
    "nxg = to_networkx(data)\n",
    "\n",
    "pagerank = networkx.pagerank(nxg) #pagerankはノードの中心性(重要性の指標)\n",
    "pagerank_max = np.array(list(pagerank.values())).max()\n",
    "\n",
    "#可視化する時のノード位置\n",
    "draw_position = networkx.spring_layout(nxg,seed=0)\n",
    "\n",
    "# 色指定\n",
    "color_map = plt.get_cmap(\"tab10\")\n",
    "labels = data.x.numpy()\n",
    "colors = [color_map(i) for i in labels]\n",
    "\n",
    "svg = SVG(networkx.nx_agraph.to_agraph(nxg).draw(prog='fdp', format='svg'))\n",
    "display(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したいラベルのインデックス位置\n",
    "target_idx = 1\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train() #訓練モード\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y[:, target_idx].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_avg_loss = epoch_loss / total_graphs\n",
    "    val_loss = 0\n",
    "    total_graphs = 0\n",
    "    model.eval()\n",
    "    for batch in valid_loader:\n",
    "        output = model(batch)\n",
    "        loss = criterion(output,batch.y[:, target_idx].unsqueeze(1)) #平方根で比較\n",
    "        val_loss += loss.item()\n",
    "        total_graphs += batch.num_graphs\n",
    "    \n",
    "    val_avg_loss = val_loss / total_graphs\n",
    "    print(f\"Epochs: {i} | epoch avg. loss: {train_avg_loss:.2f} | validation avg. loss: {val_avg_loss:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
