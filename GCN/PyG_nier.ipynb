{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import one_hot, scatter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, NNConv\n",
    "from torch_geometric.nn.conv import GATv2Conv, GATConv, TransformerConv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import BondType, HybridizationType\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size の変更\n",
    "\n",
    "RMSE or MAE\n",
    "\n",
    "反応エネルギー(熱依存)のデータセット\n",
    "\n",
    "反応エネルギーを予測するモデルの有効性を調べる（大変）\n",
    "\n",
    "大きめのデータセットを使ってみる\n",
    "\n",
    "転位の直接的な予測\n",
    "\n",
    "SphereNet使ってみる？\n",
    "\n",
    "井田先生のモデル　transformer(QM9を井田先生のグラフ構造に変換)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.graphcore.ai/posts/getting-started-with-pytorch-geometric-pyg-on-graphcore-ipus\n",
    "\n",
    "# GCN\n",
    "#NNでは64層くらい使ってる場合もある\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 32)\n",
    "        self.linear1 = nn.Linear(16,1)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        #self.conv3 = GCNConv(32, dataset.num_classes) #num_classes:ラベルの数\n",
    "    #バッチノルム(正則化)\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        # Dropout:一定割合のノードを不活性化(0になる)させ、過学習を緩和する。pはゼロになるノードの確率で、0.5がデフォルト。\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch) #これが必要やった\n",
    "        #x = F.dropout(x, p=0.2, training=self.training) # 取ってみる\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class GCN_N(torch.nn.Module):\n",
    "    def __init__(self, layer:int, dim=32):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.dim = dim\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, self.dim, improved=True)\n",
    "        self.convn = GCNConv(self.dim, self.dim, improved=True)\n",
    "        self.out = pyg.nn.Linear(self.dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        for i in range(2, self.layer + 1):\n",
    "            x = self.convn(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = pyg.nn.global_add_pool(x, batch) \n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "from statistics import pstdev\n",
    "\n",
    "targets = [\"Isotropic polarizability\", \"HOMO\", \"LUMO\", \"E_Gap\", \"Electronic spatial extent\", \"ZPVE\", \"U_0\", \"U\", \"H\", \"G\", \"Cv\", \"U_0 ATOM\", \"U ATOM\", \"H ATOM\", \"G ATOM\", \"A\", \"B\", \"C\"]\n",
    "dataset = QM9(root=\"./QM9\")\n",
    "\n",
    "#データの分割(total: 130831)\n",
    "num_train, num_val = int(len(dataset)*0.6), int(len(dataset)*0.2)\n",
    "num_test = len(dataset) - (num_train + num_val)\n",
    "batch_size = 64\n",
    "\n",
    "# 乱数の固定\n",
    "device = torch.device(\"cpu\")\n",
    "seed = 0\n",
    "pyg.seed_everything(seed=seed)\n",
    "\n",
    "train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "#Dataloaderの生成\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "\n",
    "def train(target_idx, num_epochs, mae=False):\n",
    "    #split()\n",
    "\n",
    "    target = targets[target_idx]  \n",
    "    criterion_mse = F.mse_loss\n",
    "    criterion_mae = F.l1_loss\n",
    "    if mae:\n",
    "        criterion = \"MAE\"\n",
    "    else:\n",
    "        criterion = \"RMSE\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_R2 = 0\n",
    "        train_R2_list = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(\"cpu\")\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch)\n",
    "            if mae:\n",
    "                loss = criterion_mae(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            else:\n",
    "                loss = criterion_mse(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "            # R2\n",
    "            R2 = r2_score(batch.y[:, target_idx].unsqueeze(1).detach().numpy().copy(), prediction.detach().numpy().copy())\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            train_R2 += R2\n",
    "            train_R2_list.append(R2)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /=  len(train_loader) #損失の平均(batchあたり) #平均を取ってからルート\n",
    "        if mae:\n",
    "            pass\n",
    "        else:\n",
    "            train_loss = sqrt(train_loss)   \n",
    "        \n",
    "        train_R2 /= len(train_loader)\n",
    "        train_R2_std = pstdev(train_R2_list)\n",
    "        train_R2_std /= len(train_loader)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_R2 = 0\n",
    "        valid_R2_list = []\n",
    "        total_graphs = 0\n",
    "        with torch.inference_mode(): # 自動微分無効。torch.no_grad()よりさらに高速化\n",
    "            for batch in valid_loader:\n",
    "                prediction = model(batch)\n",
    "                if mae:\n",
    "                    loss = criterion_mae(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                else:\n",
    "                    loss = criterion_mse(prediction, batch.y[:, target_idx].unsqueeze(1))\n",
    "                # R2\n",
    "                R2 = r2_score(batch.y[:, target_idx].unsqueeze(1).detach().numpy().copy(), prediction.detach().numpy().copy())\n",
    "                valid_loss += loss.item()\n",
    "                valid_R2 += R2\n",
    "                valid_R2_list.append(R2)\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "        if mae:\n",
    "            pass\n",
    "        else:\n",
    "            valid_loss = sqrt(valid_loss)\n",
    "    \n",
    "        valid_R2 /= len(valid_loader)\n",
    "        valid_R2_std = pstdev(valid_R2_list)\n",
    "        valid_R2_std /= len(valid_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | train_loss:{train_loss}, valid_loss:{valid_loss}\")\n",
    "    result = [target, criterion, train_loss, valid_loss, train_R2, valid_R2, train_R2_std, valid_R2_std]\n",
    "    return result\n",
    "\n",
    "results = [[\"target\", \"criterion\", \"train_loss\", \"valid_loss\", \"train_R2\", \"valid_R2\", \"train_R2_std\", \"valid_R2_std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "dim = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# RMSE\n",
    "for target_idx in range(len(targets)):\n",
    "    # 乱数の固定\n",
    "    device = torch.device(\"cpu\")\n",
    "    seed = 0\n",
    "    pyg.seed_everything(seed=seed)\n",
    "\n",
    "    train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "    #Dataloaderの生成\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    model = GCN_N(layer=layer,dim=dim)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "    #Dataloaderの生成\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    model = GCN_N(layer=layer,dim=dim)\n",
    "    # Optimizerの初期化\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    result = train(target_idx=target_idx, mae=False, num_epochs=num_epochs)\n",
    "    results.append(result)\n",
    "    if target_idx == 0 :\n",
    "        if not os.path.isfile(\"GCN_3_32.csv\"):\n",
    "            df_RMSE = pd.DataFrame([result], columns=results)\n",
    "            df_RMSE.to_csv(\"GCN_3_32.csv\", index=False, header=True)\n",
    "        else:\n",
    "            df_RMSE = pd.DataFrame([result], columns=results)\n",
    "            df_RMSE.to_csv(\"GCN_3_32.csv\", mode=\"a\", index=False, header=False)     \n",
    "    else:\n",
    "        df_RMSE = pd.DataFrame([result], columns=results)\n",
    "        df_RMSE.to_csv(\"GCN_3_32.csv\", mode=\"a\", index=False, header=False)  \n",
    "    \n",
    "# MAE\n",
    "for target_idx in range(len(targets)):\n",
    "    model = GCN_N(layer=layer,dim=dim)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    train_set, valid_set, test_set = random_split(dataset, [num_train, num_val, num_test])\n",
    "    #Dataloaderの生成\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=pyg.seed_everything(seed))\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, worker_init_fn=pyg.seed_everything(seed))\n",
    "    # Optimizerの初期化\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    result = train(target_idx=target_idx+1, mae=True, num_epochs=num_epochs)\n",
    "    results.append(result)\n",
    "    if target_idx == 0 :\n",
    "        if not os.path.isfile(\"GCN_3_32.csv\"):\n",
    "            df_RMSE = pd.DataFrame([result], columns=results)\n",
    "            df_RMSE.to_csv(\"GCN_3_32.csv\", index=False, header=True)\n",
    "        else:\n",
    "            df_RMSE = pd.DataFrame([result], columns=results)\n",
    "            df_RMSE.to_csv(\"GCN_3_32.csv\", mode=\"a\", index=False, header=False)     \n",
    "    else:\n",
    "        df_RMSE = pd.DataFrame([result], columns=results)\n",
    "        df_RMSE.to_csv(\"GCN_3_32.csv\", mode=\"a\", index=False, header=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
